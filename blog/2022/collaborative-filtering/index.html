<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Explicit and Implicit Collaborative Filtering | Josu Alonso Castanedo</title> <meta name="author" content="Josu Alonso Castanedo"> <meta name="description" content="How can we get to the best recommendation possible when little to no reviews are available?"> <meta name="keywords" content="machine-learning, data-science, ai"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://josumsc.github.io/blog/2022/collaborative-filtering/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Josu </span>Alonso Castanedo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Explicit and Implicit Collaborative Filtering</h1> <p class="post-meta">July 30, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> deep learning</a>   <a href="/blog/tag/pytorch"> <i class="fas fa-hashtag fa-sm"></i> pytorch</a>   <a href="/blog/tag/recommender-system"> <i class="fas fa-hashtag fa-sm"></i> recommender system</a>   <a href="/blog/tag/e-commerce"> <i class="fas fa-hashtag fa-sm"></i> e-commerce</a>   </p> </header> <article class="post-content"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/collaborative-filtering-01-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/collaborative-filtering-01-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/collaborative-filtering-01-1400.webp"></source> <img src="/assets/img/collaborative-filtering-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>There are several situations when we may want to recommend a product or a piece of content to a user in order to increase our revenue, the engagement of the user with our site or the value perceived by the user while experiencing our site. We cannot recommend items that the user has already left a review on, as he or she has already seen the product and is well-aware of its features. Therefore, we must adventure and try to estimate which items of our catalog are the ones that the user might be interested in, without having an explicit signal for that particular item/user combination.</p> <p>This is where Collaborative Filtering (CF) comes in. Collaborative Filtering is a technique that allows us to estimate the interest of a user to a set of items. The idea is that we can use the reviews of other users to estimate the interest of a different user on the items that those first users reviewed. It is similar to say:</p> <blockquote> <p>“Person A liked products X, Y and Z. Person B liked products X and Z. Therefore, B might like Y with a reasonable confidence.”.</p> </blockquote> <p>This interesting and intuitive process is done by assigning $K$ latent features for each item, and then infer for both the users and the items each of these features so we can see how the user and the item are related. These latent factors can be imagined as the implicit features that our items have, such as the “recentness” of an item in the market, how eco-friendly it is or how related is to a particular topic, such as culture or health, but in the end they are just vectors that are optimized to minimize a cost function, similarly as it happens with <em>word embeddings</em> in NLP tasks.</p> <p>In order to make these predictions and approximate the users preferences successfully we must check the data available and thus the restrictions of the models we may develop to help us with the task at hand. Depending on the data we have, there are two main branches of collaborative filtering models:</p> <ol> <li> <strong>Explicit Collaborative Filtering</strong>, which are the most common models, and the ones that are ancient and proven by the research community. To apply them we need an abundance of explicit scores on the user/item combinations, such a rating from 1 to 5 stars or a good/bad review.</li> <li> <strong>Implicit Collaborative Filtering</strong>, which are models trained without any explicit score at a user/item level. To apply them we use the signals left by the user on our sites, such as the item page views, the add-to-cart events or the purchases of the user.</li> </ol> <h2 id="explicit-collaborative-filtering">Explicit Collaborative Filtering</h2> <blockquote> <p>For an example of Explicit Collaborative Filtering please check this repository: <a href="https://github.com/josumsc/movielens-recommender" rel="external nofollow noopener" target="_blank">MovieLens Recommender</a></p> </blockquote> <p>As we mentioned before, the explicit collaborative filtering models are the ones that try to predict a known variable, which could be a continuous value as a rating from 1 to 5 stars or a categorical value as a good/bad review. Thus, we face a supervised learning problem, given a matrix X of user/item combinations and a vector y of explicit scores, and depending on the form of the scores we will be using a regressor or a classifier. Now that we know the kind of problem we are facing, it is simpler to come up with good solutions to solve it, so let’s explore a few.</p> <h3 id="classic-collaborative-filtering">Classic Collaborative Filtering</h3> <p>A few years back, when Deep Learning was not still as popular as it is and computational resources were scarce, we had to use a very simple model to predict the ratings of our users. Although, this primordial model was not bad at all, and it served as foundation to the vast majority of the newer models. This model was simply called <strong>Collaborative Filtering</strong> and it was based on the random assignment of $K$ latent factors for the matrices of users and items, and then use those matrices to infer the ratings of the users to the items by multiplying them. After that, we could compute our loss depending on the kind of label we have (either <em>MSE</em> for regression or <em>crossentropy</em> for classification, for example) and if the score was off then we could use an optimizer such as <em>SGD</em> to adjust the latent factors to the correct value. Then, we would compute again the ratings of the users to the items, finishing our training cycle.</p> <blockquote> <p>If we want a more in depth view of classic collaborative filtering, <a href="https://www.coursera.org/learn/machine-learning-course/lecture/2WoBV/collaborative-filtering" rel="external nofollow noopener" target="_blank">this video by Andrew Ng</a> is an invaluable resource.</p> </blockquote> <p>We can apply this method using simple <code class="language-plaintext highlighter-rouge">NumPy</code>, but as it is based on linear algebra and we might want to take advantage of our GPUs let’s see an example using <code class="language-plaintext highlighter-rouge">PyTorch</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Recommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">y_range</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Recommender</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">y_range</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">users</span> <span class="o">*</span> <span class="n">movies</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">result</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>As we can see, we instantiate both the weights and biases for each user and item and then simply multiply their weight matrices together. We also add the bias to the result of the multiplication, and we clip the result to the range of the ratings we have in order to make our network converge faster. We could instantiate and train the model using this snippet of code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Starting epoch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss obtained: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">n_users</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">userId</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">n_movies</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">movieId</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">emb_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Recommender</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">y_range</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</code></pre></div></div> <p>Despite its simplicity, this model should be able to predict the ratings of our users with a decent error if we tune the number of latent factors and the learning rate accordingly, although in the end it’s just a linear model and it has its drawbacks and limitations, so let’s see how we can build upon it.</p> <h3 id="deep-collaborative-filtering">Deep Collaborative Filtering</h3> <p>Breakthroughs as the adoption of GPUs by the deep learning community, the availability of immense quantities of data and new parameter initialization techniques or activation functions have awaken a new wave of interest in the field of deep learning and neural networks. This also apply to the field of recommender systems, as thousands of websites can make use of better recommendations for their users.</p> <p>Enter the <strong>feed-forward collaborative filtering models</strong>, which build upon the latent factors of the classic collaborative filtering models by adding several layers on top of them. These layers allow our models to find deeper and more complicated relationships between the users and the items, and thus, to predict the ratings of the users to the items with a better accuracy. It is important to mind that these models are nothing but an application of the ideas previously seen, which are sometimes more than enough to provide good recommendations, and that they come with a higher computational cost, so it may be wise to test first a simpler model and then iterate upon it.</p> <p>Continuing our PyTorch code, we now will see how to build this kind of models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FFRecommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">y_range</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FFRecommender</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">emb_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_units</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">y_range</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">users</span><span class="p">,</span> <span class="n">movies</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>The additions to the code come as linear layers. We also added a dropout layer as regularization and drop the matrix multiplication, as PyTorch’s <code class="language-plaintext highlighter-rouge">nn.Linear</code> does that for us. In this occasion, the code to instantiate and train the model is very similar to the one we used for the classic collaborative filtering model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_units</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">ff_model</span> <span class="o">=</span> <span class="nc">FFRecommender</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">y_range</span><span class="p">)</span>
<span class="n">ff_model</span> <span class="o">=</span> <span class="n">ff_model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">ff_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="nf">train_model</span><span class="p">(</span><span class="n">ff_model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</code></pre></div></div> <p>These models are expected to reach a better score than the previous ones due to their higher complexity, although it comes at a cost in form of more hyperparameters to tune and higher computational cost.</p> <h3 id="sequential-collaborative-filtering">Sequential Collaborative Filtering</h3> <p>Continuing our journey down the rabbit hole of neural networks, we can now build a <strong>sequential collaborative filtering model</strong>. This model is a combination of the feed-forward and the recurrent models, which allow us to think of user preferences not as a fixed vector at a user level but more as a sequence based tensor that depends on the particularities of the items reviewed and their order. The idea behind this model is to capture the effect of recent reviews of users on the next item to be scored as, for instance, a user may decrease the score of a romance movie if he or she has only been watching this genre of movies lately and grew tired of them.</p> <p>On most of the occasions, our data will be in a tabular form, so we may need first to convert them to sequences to pass through our model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">userId</span> <span class="ow">in</span> <span class="n">df_train</span><span class="p">.</span><span class="n">userId</span><span class="p">.</span><span class="nf">unique</span><span class="p">():</span>
    <span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'userId'</span><span class="p">]</span> <span class="o">==</span> <span class="n">userId</span><span class="p">].</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s">'timestamp'</span><span class="p">)</span>
    <span class="n">user_sequence</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_filtered</span><span class="p">[</span><span class="s">'movieId'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">df_filtered</span><span class="p">[</span><span class="s">'rating'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
    <span class="n">sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">user_sequence</span><span class="p">)</span>

<span class="n">cutoff_index</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">sequences_train</span><span class="p">,</span> <span class="n">sequences_test</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:</span><span class="n">cutoff_index</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[</span><span class="n">cutoff_index</span><span class="p">:]</span>
</code></pre></div></div> <p>Now that we created the sequences to serve to our model, we may use Recurrent Neural Networks (RNNs) to implement this solution. The most common RNNs are LSTMs and GRUs, and due to its higher complexity and popularity we will be using LSTMs here:</p> <blockquote> <p>For a more in depth explanation of RNNs, please refer to <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU" rel="external nofollow noopener" target="_blank">this video by MIT</a>.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SeqRecommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_items</span><span class="p">,</span> <span class="n">num_output</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">item_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_items</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_output</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_hidden</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># initialize both hidden layers
</span>        <span class="nf">return </span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)),</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">item_embeddings</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                                        <span class="n">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">rating_scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">rating_scores</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
        <span class="n">rating_scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rating_scores</span>
</code></pre></div></div> <p>Regarding the training of our model, we may again see that the code is quite similar:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_units_lstm</span> <span class="o">=</span> <span class="mi">30</span>

<span class="n">seq_model</span> <span class="o">=</span> <span class="nc">SeqRecommender</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units_lstm</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">seq_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="n">seq_model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">target_ratings</span> <span class="ow">in</span> <span class="n">sequences_train</span><span class="p">:</span>
        <span class="n">seq_model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">seq_model</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">seq_model</span><span class="p">.</span><span class="nf">init_hidden</span><span class="p">()</span>
        <span class="n">sequence_var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">sequence</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="s">'int64'</span><span class="p">)))</span>
        <span class="n">ratings_scores</span> <span class="o">=</span> <span class="nf">seq_model</span><span class="p">(</span><span class="n">sequence_var</span><span class="p">)</span>
        <span class="n">target_ratings_var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">target_ratings</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">ratings_scores</span><span class="p">,</span> <span class="n">target_ratings_var</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss obtained on epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>In case we want to make our model more complex, we may want to add a second layer of LSTM to the model, or a second linear layer after the first one. Nevertheless, I still recommend using first a simpler model, evaluate the results and build add more layers to it if necessary.</p> <h2 id="implicit-collaborative-filtering">Implicit Collaborative Filtering</h2> <blockquote> <p>We can see the code of a practical example on this kind of tasks here: <a href="https://github.com/josumsc/retrocket-implicit-recommender" rel="external nofollow noopener" target="_blank">Retail Rocket Recommender</a></p> </blockquote> <p>The methods for <em>Explicit Collaborative Filtering</em> are straight-forward, effective and widely used by industry peers. Nevertheless, they require many data points of users explicitly informing us of how aligned a particular item is to their interests, which cannot always be obtained easily due to restrictions in our website or legal issues, for instance. We mentioned in the beginning of this article that there are methods to deal with training collaborative filtering models on implicit datasets, and we will see the most renowned one: <strong>Alternating Least Squares</strong>.</p> <p>The Alternating Least Squares (ALS) method was <a href="http://yifanhu.net/PUB/cf.pdf" rel="external nofollow noopener" target="_blank">first reviewed by Yifan Hu et al</a> and describes the use of a <em>confidence</em> matrix composed by the aggregated signal of the events a particular user generated while interacting with a certain product to infer the <em>preference</em> matrix, which is the binary version of the previous matrix and determines if a user has a preference for a particular product or not. Once we have the preference matrix, we compare it with the dot product of the latent factors of our users and items and weight that score by multiplying the confidence matrix, so the combinations with more signal are the ones that add more to the cost. Lastly, we will also be including a regularization parameter $\lambda$ which will help us avoid overfitting using a similar technique to L2 regularization. In this kind of models we have to avoid overfitting as much as possible as it would lead our estimators to only repeat the combinations seen previously in the training data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/collaborative-filtering-02-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/collaborative-filtering-02-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/collaborative-filtering-02-1400.webp"></source> <img src="/assets/img/collaborative-filtering-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Using a business case of an e-commerce trying to give their users the most adequate recommendations, we could apply ALS by first computing our <em>confidence</em> matrix, weighting the different signals of the user/item interactions to a single value. We will use for this case the following signals:</p> <ol> <li> <strong>Product Page views</strong>, which will score as 1 in our confidence matrix.</li> <li> <strong>Add-to-cart events</strong>, which will score as 5.</li> <li> <strong>Transaction events</strong>, which will score as 25.</li> </ol> <p>With the difference on scores for each signal we expect to prioritize the strongest events above more frequent ones which may be deceiving, such as the page views. We can use Python to compute this confidence matrix easily given those weights:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'event'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'view'</span> <span class="k">else</span> <span class="mi">5</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'addtocart'</span> <span class="k">else</span> <span class="mi">25</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'transaction'</span> <span class="k">else</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div></div> <p>Later on, we make this matrix sparse, as it’s mostly composed by 0s and this conversion will speed up training and save us a lot of memory space that in cases with thousands of products and clients might be critical:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="s">"category"</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="nf">as_ordered</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="s">"category"</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="nf">as_ordered</span><span class="p">()</span>

<span class="n">sparse_item_user</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">])))</span>
<span class="n">sparse_user_item</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">])))</span>
</code></pre></div></div> <p>Once the matrix is built and on a sparse form, we may simply use the ALS implementation of the library <code class="language-plaintext highlighter-rouge">implicit</code> to infer the latent factors of our items and users.</p> <blockquote> <p>P.S: Although ALS is the most common method for implicit recommendations, the <code class="language-plaintext highlighter-rouge">implicit</code> library has implementations for the rest of algorithms available for this task. If you are curious about them please go ahead and visit <a href="https://implicit.readthedocs.io/en/latest/" rel="external nofollow noopener" target="_blank">its documentation</a>.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">latent_factors</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">conf_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">sparse_item_user</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="s">'double'</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">implicit</span><span class="p">.</span><span class="n">als</span><span class="p">.</span><span class="nc">AlternatingLeastSquares</span><span class="p">(</span>
    <span class="n">factors</span><span class="o">=</span><span class="n">latent_factors</span><span class="p">,</span>
    <span class="n">regularization</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="n">n_iter</span>
<span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>
</code></pre></div></div> <p><em>Et voilà!</em>, our model is trained and ready to make suggestions for our users. Not only that, but we can also see which products are most similar among them, so we can infer substitute items in case of stock outs or to suggest products that solve the same needs but provide us a higher margin. The code for both this use cases would be the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">recommend_item_to_user</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">visitorid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">recommended</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">recommend</span><span class="p">(</span><span class="n">visitorid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">[</span><span class="n">visitorid</span><span class="p">],</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">recommended</span>


<span class="k">def</span> <span class="nf">similar_items_to_item</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">itemid</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">similar</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">similar_items</span><span class="p">(</span><span class="n">itemid</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">similar</span>
</code></pre></div></div> <p>And we can call these functions as we would do with any Python module:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Choose an userid
</span><span class="n">userid</span> <span class="o">=</span> <span class="mi">97154</span>
<span class="n">recommended_items</span> <span class="o">=</span> <span class="nf">recommend_item_to_user</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">userid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Recommended items for user </span><span class="si">{</span><span class="n">userid</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">recommended_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Choose an itemid
</span><span class="n">itemid</span> <span class="o">=</span> <span class="mi">350566</span>
<span class="n">similar_items</span> <span class="o">=</span> <span class="nf">similar_items_to_item</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">itemid</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similar items to </span><span class="si">{</span><span class="n">itemid</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">similar_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusions">Conclusions</h2> <p>In this post we were able to see in detail the main mechanism behind the recommendations we often see on sites as Netflix, Amazon or Alibaba and apply different techniques to increase their capabilities or apply regularization layers as Dropout on them. We also saw how to deal with not having a good enough explicit dataset by using the signals left by our users in our site such as the purchases or the page views to infer their preferences.</p> <p>Hope this was useful and in case you have any doubt about the concepts explained on this article please do not hesitate to connect with me to further discussions. Thank you and keep on learning!</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"josumsc/josumsc.github.io","data-repo-id":"R_kgDOIyNXzw","data-category":"Announcements","data-category-id":"DIC_kwDOIyNXz84CTnxY","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Josu Alonso Castanedo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>