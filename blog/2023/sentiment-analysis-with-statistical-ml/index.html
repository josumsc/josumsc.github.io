<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Sentiment Analysis with Statistical Methods | Josu Alonso Castanedo</title> <meta name="author" content="Josu Alonso Castanedo"> <meta name="description" content="Using statistical methods to perform sentiment analysis on a dataset of reviews might be simpler than you think."> <meta name="keywords" content="machine-learning, data-science, ai"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://josumsc.github.io/blog/2023/sentiment-analysis-with-statistical-ml/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Josu </span>Alonso Castanedo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Sentiment Analysis with Statistical Methods</h1> <p class="post-meta">February 12, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/nlp"> <i class="fas fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/python"> <i class="fas fa-hashtag fa-sm"></i> python</a>   </p> </header> <article class="post-content"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sentiment-statistical-01-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sentiment-statistical-01-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sentiment-statistical-01-1400.webp"></source> <img src="/assets/img/sentiment-statistical-01.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/classic-ml-sentiment-analysis/blob/master/src/IMDB_Sentiment_Analysis.ipynb" rel="external nofollow noopener" target="_blank">This repository</a> shows a quick implementation of the concepts in this article.</p> </blockquote> <p>Given all the fuss about chatbots, large language models (LLMs) and Generative Models, I was one of those excited ML practitioners that decided to give NLP a deeper look than before and see if I could apply all those promising technologies to my career and create something useful.</p> <p>So I enrolled on the <a href="https://www.deeplearning.ai/courses/natural-language-processing-specialization/" rel="external nofollow noopener" target="_blank">Deep Learning Specialization by DeepLearning.ai</a> expecting to see how to speak with a machine to make it do my job for me. But, to my surprise, the course didn’t just jump straight ahead to those fancy technologies, but instead, it started with the basics: sentiment analysis and statistical learning.</p> <p>It was a revelation to see these concepts again after my Master’s degree. I had forgotten how simple and powerful they are when dealing with day-to-day tasks and to establish baselines that are sometimes really difficult to beat. So I decided to write this article to share my experience and hopefully help someone else to get started with NLP.</p> <h2 id="sentiment-analysis">Sentiment Analysis</h2> <p>Sentiment analysis is the task of classifying a text into a positive or negative sentiment. It is a very common task in NLP and it is a good starting point to learn about the field. In teams dealing with Customer Service, for example, sentiment analysis is used to classify customer reviews and complaints into positive or negative. This way, the team can focus on the negative reviews and improve the customer experience. In politics or brand management departments, it can be used to leverage the public opinion about a topic using social media posts.</p> <p>Sentiment analysis is one of the most basic tasks in NLP, and for that reason it’s currently widespread in industry as well as in academia. For these reasons it is a good benchmarking tool to compare different models and techniques. The current state-of-the-art is focused on using LLMs, big models trained using a lot of data and compute power. These models can often be downloaded using public hubs as <a href="https://huggingface.co/" rel="external nofollow noopener" target="_blank">HuggingFace</a> and require little to none finetuning to be used. However, when the task at hand is too specific that we cannot find an appropriate pre-trained model or the task is too small to justify the use of a big model, we can use statistical methods to perform sentiment analysis.</p> <p>In this article, we will use the <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="external nofollow noopener" target="_blank">IMDB dataset</a> from Kaggle, which contains 50,000 movie reviews from IMDB, labeled as positive or negative.</p> <h2 id="statistical-methods">Statistical Methods</h2> <p>Among the different statistical methods that can be used to perform sentiment analysis, we will focus on the following:</p> <ul> <li> <strong>Logistic Regression</strong>, a linear model that uses the sigmoid function to fit the parameters to a binary classification. This binary feature can be surpassed using <a href="https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/" rel="external nofollow noopener" target="_blank">One-vs-All or similar techniques</a> in order to fit it to K classes.</li> <li> <strong>Naive Bayes</strong>, a probabilistic model that uses Bayes’ theorem to calculate the probability of a class given a set of features. It is a very simple model that can be used to perform sentiment analysis in a very fast way.</li> </ul> <p>These methods are very simple and easy to implement, and they can be used as a baseline to compare with more complex models. In addition, they have been proven to be very effective in sentiment analysis, as they were the state-of-the-art alongside SVMs in the early 2000s.</p> <h2 id="data-preparation">Data Preparation</h2> <p>In every NLP pipeline there are different steps that need to be performed before feeding the data to the model. This is necessary as our models don’t understand the strings that compose our data, but rather numbers. So we need to transform our data into a format that our models can understand. Also, we can use this step to clean our data and remove unnecessary information, such as blank spaces, punctuation or stopwords (as prepositions and very common words).</p> <p>Apart from the tokenization (convert text to numbers) and the cleansing of noisy characters, we may also stem our words. Stemming is the process of reducing a word to its root form. For example, the words “running”, “runs”, “ran” and “run” would be reduced to “run”. This is useful as it reduces the number of features that our model needs to learn, and thus it also helps to reduce the noise in our data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-1400.webp"></source> <img src="https://devopedia.org/images/article/218/8583.1569386710.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In a nutshell, our pipeline will look like this:</p> <ol> <li>Lowering the text, so uppercase and lowercase letters are considered the same and we don’t have to deal with different representations of the same word.</li> <li>Tokenization, using blank spaces as separators we separate our strings into tokens that we may look up in a learnt mapping of <code class="language-plaintext highlighter-rouge">{string: integer}</code> often called <em>vocabulary</em>.</li> <li>Removing punctuation and stopwords, to reduce the noise.</li> <li>Stemming the words to their root form thanks to the <a href="https://tartarus.org/martin/PorterStemmer/" rel="external nofollow noopener" target="_blank">Porter Stemmer</a>, to reduce the number of features.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">stopwords</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Lowercase text
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>

    <span class="c1"># Tokenize text
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>

    <span class="c1"># Remove punctuation
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">]</span>

    <span class="c1"># Remove stopwords
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

    <span class="c1"># Stem text
</span>    <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span><span class="p">]</span>

    <span class="k">return</span> <span class="s">' '</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>
</code></pre></div></div> <hr> <h2 id="modelling">Modelling</h2> <h3 id="logistic-regression">Logistic Regression</h3> <p>The first step in implementing this method is to create a frequency table of the appearance of each token in the different classes. In this case, we will have two records for each token: one for the positive class and one for the negative class. This table will be used to calculate the probability of a class given a token, that we can later on aggregate to the probability of a class given a set of tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_word_dict</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
            <span class="c1"># define the key, which is the word and label tuple
</span>            <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            
            <span class="c1"># if the key exists in the dictionary, increment the count
</span>            <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
                <span class="n">result</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># else, if the key is new, add it to the dictionary and set the count to 1
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>After creating the frequency table, we can then transform our sentences into a matrix of $N*K$ dimensions, where $N$ is the number of sentences and $K$ is the number of classes in our problem, which will represent the sum of appearances of each token in each class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">preprocess</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">preprocess</span><span class="p">:</span>
      <span class="n">word_l</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">stopwords</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">word_l</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="c1">#bias term is set to 1
</span>    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> 
    
    <span class="c1"># loop through each token
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)]</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">preprocessed_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">preprocessed_train</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">preprocessed_test</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">preprocessed_test</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
</code></pre></div></div> <p>Now we just have to fit our <code class="language-plaintext highlighter-rouge">LogisticRegression</code> model, which will learn a bias parameter (depending on the proportion of each class in the train corpus) and the weights of each feature (the sum of appearances of each token in each class). For this model we will use the <code class="language-plaintext highlighter-rouge">sklearn</code> implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">log_model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">log_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <h3 id="naive-bayes">Naive Bayes</h3> <p>As a second traditional method, we select <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="external nofollow noopener" target="_blank">Naive Bayes</a>. This method is based on the conditional probability of an example to belong to a certain class given the probabilities of each of the tokens, which will be summarized in what’s called <em>loglikelihoods</em>. To calculate this probability, we will be using the same frequency table that we calculated for the Logistic Regression:</p> \[P(W_{pos}) = \frac{freq_{pos} + 1}{N_{pos} + V}\] \[P(W_{neg}) = \frac{freq_{neg} + 1}{N_{neg} + V}\] \[\text{loglikelihood} = \log \left(\frac{P(W_{pos})}{P(W_{neg})} \right)\] <p>Besides, a bias term is fitted which represents the overall likelihood of an example of a certain class to be draft from the corpus. This bias term is called <em>logprior</em>.</p> \[\text{logprior} = \log (D_{pos}) - \log (D_{neg})\] <p>Finally, after the calculation of these both terms we just need to add them to form the final probability of an example to belong to a particular class, and then perform a <em>softmax</em> operation to retrieve the index with the higher probability. As in this case the problem is binary, we can summarize the equation as:</p> \[p = logprior + \sum_i^N (loglikelihood_i)\] <p>Although there are several implementations of Naive Bayes in Python, given the relative simpleness of this case we can create our own functions in order to get a better grasp of the inner workings of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">):</span>
    <span class="n">loglikelihood</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()}</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># calculate N_pos, N_neg, V_pos, V_neg
</span>    <span class="n">N_pos</span> <span class="o">=</span> <span class="n">N_neg</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="c1"># if the label is positive (greater than zero)
</span>        <span class="k">if</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">N_pos</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
        <span class="c1"># else, the label is negative
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">N_neg</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
    
    <span class="n">D</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
    <span class="n">D_pos</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">D_neg</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Calculate logprior
</span>    <span class="n">logprior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">D_pos</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">D_neg</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
        <span class="c1"># get the positive and negative frequency of the word
</span>        <span class="n">freq_pos</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span> <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">freq_neg</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span> <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="c1"># calculate the probability that each word is positive, and negative
</span>        <span class="n">p_w_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_pos</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">p_w_neg</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_neg</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_neg</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="c1"># calculate the log likelihood of the word
</span>        <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p_w_pos</span> <span class="o">/</span> <span class="n">p_w_neg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span>


<span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span> <span class="o">=</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">preprocessed_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <p>Once we get our parameters <code class="language-plaintext highlighter-rouge">logprior</code> and <code class="language-plaintext highlighter-rouge">loglikelihood</code>, that represent the bias and the weights of this model, we can use them to predict the class of a new example by aggregating the scores of their tokens:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
    <span class="n">word_l</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># start by adding our logprior
</span>    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">logprior</span>

    <span class="c1"># words that are not in vocabulary simply get ignored
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">loglikelihood</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">test_naive_bayes</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">,</span> <span class="n">naive_bayes_predict</span><span class="o">=</span><span class="n">naive_bayes_predict</span><span class="p">):</span>
    <span class="n">y_hats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">test_x</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">y_hats</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_hat_i</span><span class="p">)</span>

    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">test_y</span> <span class="o">-</span> <span class="n">y_hats</span><span class="p">))</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">error</span>
    <span class="k">return</span> <span class="n">y_hats</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>As we can see, this approach is very similar than the one taken in the Logistic Regression, but it is often more efficient in terms of computational cost and in model performance. In fact, in the case of the IMDb dataset, the Naive Bayes model achieves a better accuracy than even a more complex model based on RNNs (<a href="https://github.com/josumsc/classic-ml-sentiment-analysis/blob/master/src/IMDB_Sentiment_Analysis.ipynb" rel="external nofollow noopener" target="_blank">reference</a>).</p> <h2 id="conclusion">Conclusion</h2> <p>As we just saw, the implementation of a simple model based on the frequency of the tokens in the corpus can be easily explained, implemented and trained. Furthermore, their efficiency allow us to fit these models to a large vocabulary of custom tokens to adapt them to our specific needs without the need of a large amount of data.</p> <p>We should not be narrow-minded and think that due to the emergence of far more powerful algorithms these models are not useful anymore, as they both serve as a good starting point to understand our datasets and as a baseline to compare the performance of more complex models.</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"josumsc/josumsc.github.io","data-repo-id":"R_kgDOIyNXzw","data-category":"Announcements","data-category-id":"DIC_kwDOIyNXz84CTnxY","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Josu Alonso Castanedo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>