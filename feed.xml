<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://josumsc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://josumsc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-03-05T11:53:36+00:00</updated><id>https://josumsc.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal blog about data and artificial intelligence topics. </subtitle><entry><title type="html">Fine-tuning HuggingFace Transformers for Text Classification</title><link href="https://josumsc.github.io/blog/2023/fine-tuning-huggingface/" rel="alternate" type="text/html" title="Fine-tuning HuggingFace Transformers for Text Classification"/><published>2023-03-05T01:00:00+00:00</published><updated>2023-03-05T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2023/fine-tuning-huggingface</id><content type="html" xml:base="https://josumsc.github.io/blog/2023/fine-tuning-huggingface/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title-1400.webp"/> <img src="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p>The full implementation of the project described in this post can be found on <a href="https://github.com/josumsc/fake-news-detector">GitHub</a></p> </blockquote> <p>As <a href="https://www.technologyreview.com/2023/02/16/1068695/chatgpt-chatbot-battle-search-microsoft-bing-google/">Bing and Google fight for delivering the best AI based search solution</a>, Large Language Models popularity keeps increasing. We, mere mortals far from the computing and data repositories available to the big tech companies, can also take advantage of these models to solve our own problems. In this post, we will see how to fine-tune a HuggingFace Transformer model to leverage the work of those giants and create our own text classification model, with SOTA results.</p> <p>Furthermore, we will use Flask to create an API that serves our model predictions, and implement MLOps best practices to deploy our model in production and ensure a CD flow.</p> <h2 id="fine-tuning-the-model">Fine-Tuning the Model</h2> <h3 id="the-transformers-ecosystem">The Transformers ecosystem</h3> <p>HuggingFace is the go-to company for Natural Language Processing (NLP) tasks. They have a wide range of models, datasets, and tools to help you solve your NLP problems. They also have a great community that is always willing to help. They started their journey with the <a href="https://huggingface.co/docs/transformers/index">Transformers library</a> but nowadays they also provide a <a href="https://huggingface.co/models">Hub to download pretrained models and tokenizers</a>, a <a href="https://huggingface.co/datasets">Datasets library</a> to download and process datasets, and <a href="https://huggingface.co/spaces">access to spaces to train your model on them and show demos of their capabilities</a>.</p> <p>In this post, we will use the <code class="language-plaintext highlighter-rouge">datasets</code> library to download a dataset that fits our needs, the <code class="language-plaintext highlighter-rouge">transformers</code> library to download a pretrained LLM, and the Hub to upload our model and tokenizer to be able to use them in our API.</p> <p>To work with the HuggingFace ecosystem and combine it with PyTorch, we have defined a <code class="language-plaintext highlighter-rouge">DetectorPipeline</code> class that will serve as an interface to interact with our model pipeline:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DetectorPipeline</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"GonzaloA/fake_news"</span><span class="p">,</span>
        <span class="n">checkpoint</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"distilbert-base-uncased-finetuned-sst-2-english"</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"fake_news_detector"</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="s">"""Detector pipeline class.

        :param dataset_name: Name of the dataset to download, defaults to "GonzaloA/fake_news"
        :type dataset_name: str, optional
        :param checkpoint: Name of the model to fine-tune, defaults to "distilbert-base-uncased-finetuned-sst-2-english"
        :type checkpoint: str, optional
        :param model_name: Name of the model to save, defaults to "fake_news_detector"
        :type model_name: str, optional
        """</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dataset_name</span> <span class="o">=</span> <span class="n">dataset_name</span>
        <span class="n">self</span><span class="p">.</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">checkpoint</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
</code></pre></div></div> <p>The functions shown later on this part of the post will be defined as methods of this class.</p> <h3 id="the-dataset">The Dataset</h3> <p>We will use the <a href="https://huggingface.co/datasets/GonzaloA/fake_news">Fake News Dataset</a> that includes approximately 40k news articles with their header and corpus labeled as either fake or real. The dataset is already split into train, validation, and test sets, so we can download it directly from the Hub with those splits already specified.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">download_dataset</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">datasets</span><span class="p">.</span><span class="n">DatasetDict</span><span class="p">:</span>
    <span class="s">"""Download dataset from HuggingFace datasets library.

    :return: DatasetDict object with the train, validation and test splits.
    :rtype: datasets.DatasetDict
    """</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_dataset</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dataset_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div></div> <h3 id="the-model">The Model</h3> <p>With the dataset on memory, the next step would be to download the model. In this case, we will be using the <code class="language-plaintext highlighter-rouge">distilbert-base-uncased-finetuned-sst-2-english</code> model, which is a fine-tuned version of the <code class="language-plaintext highlighter-rouge">distilbert-base-uncased</code> model for the SST-2 dataset. The SST-2 dataset is a binary classification dataset with the goal of predicting whether a sentence is positive or negative. The model was trained on the SST-2 dataset and then fine-tuned on the <code class="language-plaintext highlighter-rouge">fake_news</code> dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_tokenizer_and_model</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">]:</span>
    <span class="s">"""Get tokenizer and model from model name.

    :param checkpoint: Name of the model to fine-tune, defaults to None
    :type checkpoint: str, optional
    :return: Tokenizer and Model objects.
    :rtype: (AutoTokenizer, AutoModelForSequenceClassification)
    """</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">checkpoint</span> <span class="k">if</span> <span class="n">checkpoint</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">checkpoint</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span>
</code></pre></div></div> <h3 id="the-dataloaders">The DataLoaders</h3> <p>Text datasets can be huge, and even though we were able to load them in memory it doesn’t mean that we will be able to load them into our GPU all at once, given that our model and logits also have to coexist on that instance. To solve this problem, we will use PyTorch’s <code class="language-plaintext highlighter-rouge">DataLoader</code> class to load the data in batches, passing the text by the tokenizer first to receive them as integers.</p> <p>But first, we need to define a <a href="https://huggingface.co/docs/transformers/main_classes/data_collator">DataCollator</a> that will be in charge of padding the sequences to the same length, which in this case will be the longer sequence. We will use the <code class="language-plaintext highlighter-rouge">DataCollatorWithPadding</code> class for this task.</p> <p>Also, please note that some preprocessing is done on the dataset to remove the string columns after the tokenization process and to rename the target column to <code class="language-plaintext highlighter-rouge">labels</code> to match the model’s expected input.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_data_collator</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">:</span> <span class="n">AutoTokenizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataCollatorWithPadding</span><span class="p">:</span>
    <span class="s">"""Get data collator from tokenizer.

    :param tokenizer: Tokenizer object.
    :type tokenizer: AutoTokenizer
    :return: Data collator object.
    :rtype: DataCollatorWithPadding
    """</span>

    <span class="n">data_collator</span> <span class="o">=</span> <span class="nc">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_collator</span>

<span class="k">def</span> <span class="nf">get_dataloaders</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="p">.</span><span class="n">DatasetDict</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="p">:</span> <span class="n">DataCollatorWithPadding</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataLoader</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">]:</span>
    <span class="s">"""Get the dataloaders for train, validation and test splits.

    :param dataset: Train dataset.
    :type dataset: datasets.DatasetDict
    :param batch_size: Batch size.
    :type batch_size: int
    :param tokenizer: Tokenizer object.
    :type DataLoader: AutoTokenizer
    :param data_collator: Data collator object.
    :type data_collator: DataCollatorWithPadding
    :return: Dataloaders for train, validation and test splits.
    :rtype: (DataLoader, DataLoader, DataLoader)
    """</span>

    <span class="c1"># Tokenize dataset
</span>    <span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="s">"text"</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Put in format that the model expects
</span>    <span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">tokenized_dataset</span><span class="p">.</span><span class="nf">remove_columns</span><span class="p">(</span>
        <span class="p">[</span><span class="s">"Unnamed: 0"</span><span class="p">,</span> <span class="s">"title"</span><span class="p">,</span> <span class="s">"text"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">tokenized_dataset</span> <span class="o">=</span> <span class="n">tokenized_dataset</span><span class="p">.</span><span class="nf">rename_column</span><span class="p">(</span><span class="s">"label"</span><span class="p">,</span> <span class="s">"labels"</span><span class="p">)</span>
    <span class="n">tokenized_dataset</span><span class="p">.</span><span class="nf">set_format</span><span class="p">(</span><span class="s">"torch"</span><span class="p">)</span>

    <span class="c1"># Create dataloaders
</span>    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">tokenized_dataset</span><span class="p">[</span><span class="s">"train"</span><span class="p">],</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">tokenized_dataset</span><span class="p">[</span><span class="s">"validation"</span><span class="p">],</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">test_dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">tokenized_dataset</span><span class="p">[</span><span class="s">"test"</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">data_collator</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">test_dataloader</span>
</code></pre></div></div> <h3 id="the-trainer">The Trainer</h3> <p>Once we have the dataset, the model, and the dataloaders, we can start training the model. To do this, we will use PyTorch interfaces to define the training loop and the evaluation loop:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
    <span class="n">train_dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">:</span>
    <span class="s">"""Train model.

    :param model: Model to train.
    :type model: AutoModelForSequenceClassification
    :param train_dataloader: Dataloader with train data.
    :type train_dataloader: DataLoader
    :param epochs: Number of epochs to train, defaults to 3
    :type epochs: int, optional
    :param lr: Learning rate, defaults to 2e-5
    :type lr: float, optional
    :param weight_decay: Weight decay, defaults to 0.0
    :type weight_decay: float, optional
    :param warmup_steps: Number of warmup steps, defaults to 0
    :type warmup_steps: int, optional
    :param max_grad_norm: Maximum gradient norm, defaults to 1.0
    :type max_grad_norm: float, optional
    :return: Trained model.
    :rtype: AutoModelForSequenceClassification
    """</span>

    <span class="n">num_training_steps</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">*</span> <span class="n">epochs</span>

    <span class="c1"># Set device
</span>    <span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set optimizer
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

    <span class="c1"># Set scheduler
</span>    <span class="n">scheduler</span> <span class="o">=</span> <span class="nf">get_scheduler</span><span class="p">(</span>
        <span class="s">"linear"</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
        <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Train
</span>    <span class="n">pbar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">))</span>
    <span class="k">with</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span>
                <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">max_grad_norm</span><span class="p">)</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
                <span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
                <span class="n">pbar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">datasets</span><span class="p">.</span><span class="n">Dataset</span><span class="p">,</span>
    <span class="n">dataloader</span><span class="p">:</span> <span class="n">DataLoader</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="s">"""Evaluate model.

    :param eval_dataloader: dataset to evaluate.
    :type eval_dataloader: DatasetDict
    :param dataset: Dataloader with eval data.
    :type dataset: DataLoader
    :param model: Model to evaluate.
    :type model: AutoModelForSequenceClassification
    :return: Accuracy.
    :rtype: float
    """</span>

    <span class="c1"># Set device
</span>    <span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Evaluate
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span>
        <span class="n">running_predictions</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">predictions</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">running_predictions</span><span class="p">)</span>

    <span class="c1"># Print results
</span>    <span class="nf">print</span><span class="p">(</span><span class="s">"Results of the model:</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    <span class="n">f1score</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">"validation"</span><span class="p">][</span><span class="s">"label"</span><span class="p">],</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">"macro"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"F1 score: </span><span class="si">{</span><span class="n">f1score</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">"validation"</span><span class="p">][</span><span class="s">"label"</span><span class="p">],</span> <span class="n">predictions</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">"validation"</span><span class="p">][</span><span class="s">"label"</span><span class="p">],</span> <span class="n">predictions</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">f1score</span>
</code></pre></div></div> <h3 id="putting-everything-together">Putting everything together</h3> <p>So far we have the different methods that should be called from the pipeline, although we still lack the pipeline itself. The pipeline will be the one that will call the different methods:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_pipeline</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2e-5</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">:</span>
    <span class="s">"""Runs the train pipeline and returns the trained model.
    :param epochs: Number of epochs to train, defaults to 3
    :type epochs: int, optional
    :param lr: Learning rate, defaults to 2e-5
    :type lr: float, optional
    :param batch_size: Batch size, defaults to 16
    :type batch_size: int, optional
    :return: Trained model.
    :rtype: AutoModelForSequenceClassification
    """</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">download_dataset</span><span class="p">()</span>
    <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_tokenizer_and_model</span><span class="p">()</span>
    <span class="n">data_collator</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_data_collator</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
    <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_dataloaders</span><span class="p">(</span>
        <span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">evaluate_model</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">save_pretrained</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s">"models"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model_name</span><span class="p">))</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="nf">save_pretrained</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s">"models"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model_name</span><span class="p">))</span>

    <span class="k">return</span>
</code></pre></div></div> <p>Notice how the pipeline ends by calling the <code class="language-plaintext highlighter-rouge">save_pretrained</code> method of the model and tokenizer. This will save the model and tokenizer in the <code class="language-plaintext highlighter-rouge">models</code> directory with the name defined in the class instantiation, so that we can use them later.</p> <p>After training, we will need to be able to predict the results of a given text. For this procedure, we will need to load the model and tokenizer that we have previously saved and create a <code class="language-plaintext highlighter-rouge">predict</code> function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_model_from_directory</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">]:</span>
    <span class="s">"""Load model from directory.

    :param model_name: Name of the model to load, if None, self.model_name is used, defaults to None.
    :type model_name: str
    :return: Loaded tokenizer and model.
    :rtype: (AutoTokenizer, AutoModelForSequenceClassification
    """</span>
    <span class="n">load_path</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s">"models"</span><span class="p">,</span> <span class="n">model_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model_name</span>
        <span class="k">else</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="s">"models"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">model_name</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">load_path</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="n">load_path</span><span class="p">,</span> <span class="n">local_files_only</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="s">"""Predict class of text.

    :param tokenizer: Tokenizer to use for prediction.
    :type tokenizer: AutoTokenizer
    :param model: Model to use for prediction.
    :type model: AutoModelForSequenceClassification
    :param text: Text to predict.
    :type text: str
    :return: Predicted class.
    :rtype: int
    """</span>
    <span class="c1"># Set device
</span>    <span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cuda"</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Predict
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">encoded_text</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
        <span class="n">text</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s">"pt"</span>
    <span class="p">)</span>
    <span class="n">encoded_text</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">encoded_text</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_text</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">).</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">logits</span>
</code></pre></div></div> <p>Finally, once we have the model and tokenizer loaded, we can publish them into HuggingFace Hub. For this, we will need to create another function in our pipeline:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">publish_model_from_directory</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="s">"""Publish model to Hugging Face Hub from the specified directory.
    Both the model in the directory and the model on the Hub must have the same name.
    :param model_name: Name of the model to publish, if None, self.model_name is used, defaults to None.
    :type model_name: str
    :return: None
    :rtype: None
    """</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span> <span class="k">if</span> <span class="n">model_name</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">model_name</span>
    <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">load_model_from_directory</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="nf">push_to_hub</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">push_to_hub</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">None</span>
</code></pre></div></div> <p>We have to consider that in order to publish models to the Hub we need to be logged in. For this, we can use the <code class="language-plaintext highlighter-rouge">huggingface-cli login</code> command (more info to be found in <a href="https://huggingface.co/docs/huggingface_hub/quick-start#login">their documentation</a>).</p> <h2 id="creating-a-cli-to-interact-with-the-model">Creating a CLI to interact with the model</h2> <p>Even though we have simplified the process of loading, training and predicting using our pipeline, we would still need to import the class and call the different methods every time we would want to use it. For this reason, we will create a CLI that will allow us to interact with the model without having to write any unnecessary code, increasing our efficiency and avoiding errors.</p> <p>We will be using the <code class="language-plaintext highlighter-rouge">click</code> library to create the CLI. The first step is to create a <code class="language-plaintext highlighter-rouge">cli.py</code> file that will contain the CLI:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Description: Command line interface for the fake news detector.
</span><span class="kn">import</span> <span class="n">click</span>


<span class="nd">@click.group</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">cli</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="nf">cli</span><span class="p">()</span> 
</code></pre></div></div> <p>This will create the command line, but for now we have nothing to do with it. We need to add the different methods to the CLI by using the <code class="language-plaintext highlighter-rouge">@click.command()</code> decorator and the <code class="language-plaintext highlighter-rouge">cli.add_command()</code> method. We will start by adding the <code class="language-plaintext highlighter-rouge">train</code> command:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">detector</span> <span class="kn">import</span> <span class="n">DetectorPipeline</span>

<span class="nd">@click.command</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"train"</span><span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span>
    <span class="s">"--dataset"</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="s">"GonzaloA/fake_news"</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s">"Dataset to download from HuggingFace datasets library."</span><span class="p">,</span>
<span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span>
    <span class="s">"--checkpoint"</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="s">"distilbert-base-uncased-finetuned-sst-2-english"</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s">"Model to fine-tune."</span><span class="p">,</span>
<span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span>
    <span class="s">"--output"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"fake-news-detector"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Name of the model to save."</span>
<span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span><span class="s">"--lr"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Learning rate."</span><span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span><span class="s">"--batch_size"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Batch size."</span><span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span><span class="s">"--epochs"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Number of epochs."</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">):</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="nc">DetectorPipeline</span><span class="p">(</span>
        <span class="n">dataset_name</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="n">output</span>
    <span class="p">)</span>
    <span class="n">pipeline</span><span class="p">.</span><span class="nf">train_pipeline</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="n">click</span><span class="p">.</span><span class="nf">echo</span><span class="p">(</span><span class="sa">f</span><span class="s">"Model saved into directory ./models/</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
</code></pre></div></div> <p>By looking careful we can see how <code class="language-plaintext highlighter-rouge">click</code> deals with CLI arguments, which are passed to the function as parameters. This syntax based on decorators can be seen weird in the beginning but help us abstract those functionalities from the function to be defined. We can also appreciate the <code class="language-plaintext highlighter-rouge">click.echo</code> method, that will inform us of the finishing of the training process and the saving directory.</p> <p>Finally, we can declare the <code class="language-plaintext highlighter-rouge">predict</code> and <code class="language-plaintext highlighter-rouge">publish</code> commands and calling the <code class="language-plaintext highlighter-rouge">cli.add_command()</code> method to add these new functions to the CLI:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@click.command</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"predict"</span><span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span>
    <span class="s">"--model"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"fake-news-detector"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Model to use for prediction."</span>
<span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span>
    <span class="s">"--checkpoint"</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="s">"distilbert-base-uncased-finetuned-sst-2-english"</span><span class="p">,</span>
    <span class="n">help</span><span class="o">=</span><span class="s">"Tokenizer used to tokenize the text."</span><span class="p">,</span>
<span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span><span class="s">"--text"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"This is a fake news"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Text to predict."</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="nc">DetectorPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="nf">load_model_from_directory</span><span class="p">()</span>
    <span class="n">prediction</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">click</span><span class="p">.</span><span class="nf">echo</span><span class="p">(</span><span class="sa">f</span><span class="s">"Prediction: </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="n">click</span><span class="p">.</span><span class="nf">echo</span><span class="p">(</span><span class="sa">f</span><span class="s">"Logits: </span><span class="si">{</span><span class="n">logits</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="nd">@click.command</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"publish"</span><span class="p">)</span>
<span class="nd">@click.option</span><span class="p">(</span><span class="s">"--model"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s">"fake-news-detector"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Model to publish."</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">publish</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="nc">DetectorPipeline</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
    <span class="n">pipeline</span><span class="p">.</span><span class="nf">publish_model_from_directory</span><span class="p">()</span>
    <span class="n">click</span><span class="p">.</span><span class="nf">echo</span><span class="p">(</span><span class="s">"Model published into HuggingFace Hub."</span><span class="p">)</span>

<span class="n">cli</span><span class="p">.</span><span class="nf">add_command</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
<span class="n">cli</span><span class="p">.</span><span class="nf">add_command</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
<span class="n">cli</span><span class="p">.</span><span class="nf">add_command</span><span class="p">(</span><span class="n">publish</span><span class="p">)</span>
</code></pre></div></div> <p>Now whenever we want to interact with the model, we can simply run the <code class="language-plaintext highlighter-rouge">cli.py</code> file and use the different commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python cli.py train <span class="nt">--epochs</span> 1
python cli.py predict <span class="nt">--text</span> <span class="s2">"This is a fake news"</span>
python cli.py publish
</code></pre></div></div> <h2 id="creating-a-rest-api">Creating a REST API</h2> <p>Having a CLI tool is great to work on your server or local machine, but it is not very convenient to use it in a production environment. Web or app teams should need to SSH to the server to run the commands, which will add a lot of complexity and delays in the communication, so it’s not a proper solution to serve predictions to the final user.</p> <p>For this reason, we will create a REST API that will allow us to interact with the model using HTTP requests. We will be using the <code class="language-plaintext highlighter-rouge">flask</code> library to create the API. The first step is to create a <code class="language-plaintext highlighter-rouge">app.py</code> file that will contain the code needed to run the server:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">flask</span> <span class="kn">import</span> <span class="n">Flask</span><span class="p">,</span> <span class="n">render_template</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">jsonify</span>
<span class="kn">from</span> <span class="n">detector</span> <span class="kn">import</span> <span class="n">DetectorPipeline</span>

<span class="n">app</span> <span class="o">=</span> <span class="nc">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>

<span class="nd">@app.route</span><span class="p">(</span><span class="s">"/"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">index</span><span class="p">():</span>
    <span class="k">return</span> <span class="nf">render_template</span><span class="p">(</span><span class="s">"index.html"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">app</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s">"0.0.0.0"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</code></pre></div></div> <p>This will create a simple Flask server on the port 5000 open for traffic access from every IP (<em>care with this in a real production environment!</em>) that will serve the <code class="language-plaintext highlighter-rouge">index.html</code> file when we access the root of the server. We will create this file in the <code class="language-plaintext highlighter-rouge">templates</code> folder:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
    <span class="nt">&lt;head&gt;</span>
        <span class="nt">&lt;title&gt;</span>Fake news detector<span class="nt">&lt;/title&gt;</span>
    <span class="nt">&lt;/head&gt;</span>
    <span class="nt">&lt;body&gt;</span>
        <span class="nt">&lt;h1&gt;</span>Powered by HuggingFace and PyTorch<span class="nt">&lt;/h1&gt;</span>
        <span class="nt">&lt;p&gt;</span>This app is conceived to be used as an API REST, although this particular endpoint serves as an entrypoint where we can test the functionalities using a form.<span class="nt">&lt;/p&gt;</span>
        <span class="nt">&lt;form</span> <span class="na">name=</span><span class="s">"input"</span> <span class="na">action=</span><span class="s">"/detect_html"</span> <span class="na">method=</span><span class="s">"post"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;label</span> <span class="na">for=</span><span class="s">"text"</span><span class="nt">&gt;</span>Text to detect:<span class="nt">&lt;/label&gt;</span>
            <span class="nt">&lt;input</span> <span class="na">type=</span><span class="s">"text"</span> <span class="na">id=</span><span class="s">"text"</span> <span class="na">name=</span><span class="s">"text"</span> <span class="na">value=</span><span class="s">"The president of the United States is Donald Trump."</span><span class="nt">&gt;</span>
            <span class="nt">&lt;input</span> <span class="na">type=</span><span class="s">"submit"</span> <span class="na">value=</span><span class="s">"Submit"</span><span class="nt">&gt;</span>
        <span class="nt">&lt;/form&gt;</span>
        
    <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</code></pre></div></div> <p>The main functionality of our app will be to be used in a simulated production environment by serving HTTP requests, but it’s a good practice to serve also an index at route <code class="language-plaintext highlighter-rouge">/</code> to verify the correct functioning of the server. Furthermore, in this example we can use Jinja templates to pass parameters to the HTML file and render them in the browser. These parameters will be the result of the prediction and the logits, which will be passed to the template using the following function on the <code class="language-plaintext highlighter-rouge">app.py</code> file:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@app.route</span><span class="p">(</span><span class="s">"/detect_html"</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s">"POST"</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">detect_html</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">request</span><span class="p">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">"POST"</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">form</span><span class="p">[</span><span class="s">"text"</span><span class="p">]</span>
        <span class="n">result</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">render_template</span><span class="p">(</span><span class="s">"index.html"</span><span class="p">,</span> <span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/html-interface?raw=true-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/html-interface?raw=true-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/html-interface?raw=true-1400.webp"/> <img src="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/html-interface.png?raw=true" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Now that we have dealt with the HTML part of the server, we can create the endpoint that will be used to serve the predictions. We will create a <code class="language-plaintext highlighter-rouge">detect_json</code> function that will be called when we send a POST request to the <code class="language-plaintext highlighter-rouge">/detect_json</code> endpoint:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@app.route</span><span class="p">(</span><span class="s">"/detect_json"</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s">"POST"</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">detect_json</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">request</span><span class="p">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">"POST"</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">[</span><span class="s">"text"</span><span class="p">]</span>
        <span class="n">result</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">jsonify</span><span class="p">(</span><span class="n">result</span><span class="o">=</span><span class="n">result</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/api-rest?raw=true-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/api-rest?raw=true-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/api-rest?raw=true-1400.webp"/> <img src="https://github.com/josumsc/fake-news-detector/blob/master/docs/img/api-rest.png?raw=true" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To run our server and test the different endpoints, we can simply run the <code class="language-plaintext highlighter-rouge">app.py</code> file:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python app.py
</code></pre></div></div> <h2 id="deploying-the-model-to-production">Deploying the model to production</h2> <p>We now have a proper API to serve predictions and a CLI command to train and publish the model, great! But, let’s imagine for a moment that our production team decides to change the server where the model is running. They would need to configure everything to run the flask API again, which would imply some downtime and a lot of work. Even if we prepare for this, sometimes a small change in the libraries installed in the server could mean a total failure of the API. For this reason, it is very important to have a proper deployment strategy to avoid these problems.</p> <p>The deployment strategy we will follow is based on using <strong>Docker</strong> as a containerization tool to create a container with all the dependencies and the code needed to run the API.</p> <blockquote> <p>In a real production environment, we could also add this container to a tool like <strong>Kubernetes</strong> to ensure high availability and scalability of the API, but as our API is very simple, we will not add this complexity to the example.</p> </blockquote> <p>To create this container, we will create a <code class="language-plaintext highlighter-rouge">Dockerfile</code> file that will contain the instructions to build the image:</p> <div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.9</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">COPY</span><span class="s"> requirements.txt .</span>
<span class="k">COPY</span><span class="s"> src/ .</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">--no-install-recommends</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    libpq-dev <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="k">EXPOSE</span><span class="s"> 5000</span>
<span class="k">ENTRYPOINT</span><span class="s"> ["python"]</span>
<span class="k">CMD</span><span class="s"> ["app.py"]</span>
</code></pre></div></div> <p>We can see how the <code class="language-plaintext highlighter-rouge">python:3.9</code> image is downloaded from Docker Hub and used as a base image to build our own image. Then, we copy the <code class="language-plaintext highlighter-rouge">requirements.txt</code> file and the <code class="language-plaintext highlighter-rouge">src</code> folder to the container and install the dependencies. Finally, we expose the port 5000 and set the <code class="language-plaintext highlighter-rouge">app.py</code> file as the entrypoint of the container. To build the image we can run the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> fake-news-detector <span class="nb">.</span>
</code></pre></div></div> <p>Once our container is built we can share it with our production team and they will be able to run it without any problem. In the example of Docker Hub, publishing the container is as simple as running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker push fake-news-detector
</code></pre></div></div> <p>Now we can run the container in our production server using the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-p</span> 5001:5000 fake-news-detector
</code></pre></div></div> <p>Or even better, use <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file to run the container with the rest of the services in case extra dependencies are added:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># docker-compose.yml</span>
<span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.9"</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">app</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span> <span class="s">.</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">5001:5000"</span>
    <span class="na">restart</span><span class="pi">:</span>
      <span class="s">always</span>
</code></pre></div></div> <p>This way we can run and stop the container with the following commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose <span class="nt">-f</span> docker-compose.yml up <span class="nt">-d</span> <span class="nt">--remove-orphans</span> <span class="nt">--build</span> <span class="nt">--force-recreate</span>
docker-compose <span class="nt">-f</span> docker-compose.yml down
</code></pre></div></div> <h2 id="extra-makefile">Extra: Makefile</h2> <p>We have everything we need now to train our model, create our API and share it with the world without thinking about compatibilities or requirements, and that’s a really good job done! But, we have skipped some important nuances of the MLOps cycle, such as linting the code or formatting, and we have left some manual steps that could be automated. For this reason, we will create a <code class="language-plaintext highlighter-rouge">Makefile</code> file to automate some of these tasks:</p> <div class="language-makefile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Makefile
</span><span class="nv">DOCKER_USERNAME</span> <span class="o">=</span> josumsc

<span class="nl">install</span><span class="o">:</span>
	pip <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span><span class="se">\</span>
		pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="nl">format</span><span class="o">:</span>
	black src/<span class="k">*</span>.py

<span class="nl">lint</span><span class="o">:</span>
	pylint <span class="nt">--disable</span><span class="o">=</span>R,C,W1203,E1101 src/.<span class="k">*</span>
	docker run <span class="nt">--rm</span> <span class="nt">-i</span> hadolint/hadolint &lt; Dockerfile

<span class="nl">publish</span><span class="o">:</span>
	python src/cli.py publish
	docker build <span class="nt">-t</span> <span class="p">$(</span>DOCKER_USERNAME<span class="p">)</span>/flask-fake-news:latest .
	docker push <span class="p">$(</span>DOCKER_USERNAME<span class="p">)</span>/flask-fake-news:latest

<span class="nl">run</span><span class="o">:</span>
	docker-compose <span class="nt">-f</span> docker-compose.yml up <span class="nt">-d</span> <span class="nt">--remove-orphans</span> <span class="nt">--build</span> <span class="nt">--force-recreate</span>
	<span class="p">@</span><span class="nb">echo</span> <span class="s2">"App deployed at http://localhost:5001"</span>

<span class="nl">stop</span><span class="o">:</span>
	docker-compose <span class="nt">-f</span> docker-compose.yml down
</code></pre></div></div> <p>We can see how we have added some commands to install the dependencies, format the code, lint the code and Dockerfile, publish the model and run the container. We can run these commands with the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make &lt;<span class="nb">command</span><span class="o">&gt;</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>By following the steps in this article, we have created a simple API to serve predictions and a CLI command to train and publish the model. We have also created a Dockerfile to build a container that automatically serves our API. Finally, we have created a Makefile to automate some of the tasks and make the development process easier.</p> <p>NLP and MLOps are 2 concepts that the modern Machine Learning Engineer should master to make the most of machine learning applications. Fortunately, the most groundbreaking tools are also open source, so we can read their documentation and use them to create powerful applications as the one shown in this article.</p> <p>I hope that this was useful to some of you, have fun and happy coding!</p> <blockquote> <p>As next step, we could create a GitHub Actions pipeline to automatically publish the model in Docker whenever we push to master, but as we have seen, the process is very simple and we can do it manually without any problem. In case your project expects to have a long lifetime, please do consider adding a CI/CD pipeline to automate the process by <a href="https://docs.github.com/en/actions/publishing-packages/publishing-docker-images#publishing-images-to-docker-hub">following the steps here</a>.</p> </blockquote> <h2 id="references">References</h2> <ul> <li><a href="https://huggingface.co/transformers/index.html">HuggingFace Transformers</a></li> <li><a href="https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/">Natural Language Processing with Transformers Revised Edition</a></li> <li><a href="https://learning.oreilly.com/library/view/deploy-machine-learning/9781484265468/">Deploy Machine Learning Models to Production: With Flask, Streamlit, Docker, and Kubernetes on Google Cloud Platform</a></li> <li><a href="https://www.coursera.org/learn/open-source-mlops-platforms-duke/home/week/1">Open Source Platforms for MLOps</a></li> </ul>]]></content><author><name></name></author><category term="nlp"/><category term="python"/><category term="mlops"/><category term="pytorch"/><category term="deep learning"/><summary type="html"><![CDATA[Creating your own text classification API using HuggingFace Transformers and Flask]]></summary></entry><entry><title type="html">Sentiment Analysis with Statistical Methods</title><link href="https://josumsc.github.io/blog/2023/sentiment-analysis-with-statistical-ml/" rel="alternate" type="text/html" title="Sentiment Analysis with Statistical Methods"/><published>2023-02-12T01:00:00+00:00</published><updated>2023-02-12T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2023/sentiment-analysis-with-statistical-ml</id><content type="html" xml:base="https://josumsc.github.io/blog/2023/sentiment-analysis-with-statistical-ml/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sentiment-statistical-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sentiment-statistical-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sentiment-statistical-01-1400.webp"/> <img src="/assets/img/sentiment-statistical-01.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/classic-ml-sentiment-analysis/blob/master/src/IMDB_Sentiment_Analysis.ipynb">This repository</a> shows a quick implementation of the concepts in this article.</p> </blockquote> <p>Given all the fuss about chatbots, large language models (LLMs) and Generative Models, I was one of those excited ML practitioners that decided to give NLP a deeper look than before and see if I could apply all those promising technologies to my career and create something useful.</p> <p>So I enrolled on the <a href="https://www.deeplearning.ai/courses/natural-language-processing-specialization/">Deep Learning Specialization by DeepLearning.ai</a> expecting to see how to speak with a machine to make it do my job for me. But, to my surprise, the course didn’t just jump straight ahead to those fancy technologies, but instead, it started with the basics: sentiment analysis and statistical learning.</p> <p>It was a revelation to see these concepts again after my Master’s degree. I had forgotten how simple and powerful they are when dealing with day-to-day tasks and to establish baselines that are sometimes really difficult to beat. So I decided to write this article to share my experience and hopefully help someone else to get started with NLP.</p> <h2 id="sentiment-analysis">Sentiment Analysis</h2> <p>Sentiment analysis is the task of classifying a text into a positive or negative sentiment. It is a very common task in NLP and it is a good starting point to learn about the field. In teams dealing with Customer Service, for example, sentiment analysis is used to classify customer reviews and complaints into positive or negative. This way, the team can focus on the negative reviews and improve the customer experience. In politics or brand management departments, it can be used to leverage the public opinion about a topic using social media posts.</p> <p>Sentiment analysis is one of the most basic tasks in NLP, and for that reason it’s currently widespread in industry as well as in academia. For these reasons it is a good benchmarking tool to compare different models and techniques. The current state-of-the-art is focused on using LLMs, big models trained using a lot of data and compute power. These models can often be downloaded using public hubs as <a href="https://huggingface.co/">HuggingFace</a> and require little to none finetuning to be used. However, when the task at hand is too specific that we cannot find an appropriate pre-trained model or the task is too small to justify the use of a big model, we can use statistical methods to perform sentiment analysis.</p> <p>In this article, we will use the <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">IMDB dataset</a> from Kaggle, which contains 50,000 movie reviews from IMDB, labeled as positive or negative.</p> <h2 id="statistical-methods">Statistical Methods</h2> <p>Among the different statistical methods that can be used to perform sentiment analysis, we will focus on the following:</p> <ul> <li><strong>Logistic Regression</strong>, a linear model that uses the sigmoid function to fit the parameters to a binary classification. This binary feature can be surpassed using <a href="https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/">One-vs-All or similar techniques</a> in order to fit it to K classes.</li> <li><strong>Naive Bayes</strong>, a probabilistic model that uses Bayes’ theorem to calculate the probability of a class given a set of features. It is a very simple model that can be used to perform sentiment analysis in a very fast way.</li> </ul> <p>These methods are very simple and easy to implement, and they can be used as a baseline to compare with more complex models. In addition, they have been proven to be very effective in sentiment analysis, as they were the state-of-the-art alongside SVMs in the early 2000s.</p> <h2 id="data-preparation">Data Preparation</h2> <p>In every NLP pipeline there are different steps that need to be performed before feeding the data to the model. This is necessary as our models don’t understand the strings that compose our data, but rather numbers. So we need to transform our data into a format that our models can understand. Also, we can use this step to clean our data and remove unnecessary information, such as blank spaces, punctuation or stopwords (as prepositions and very common words).</p> <p>Apart from the tokenization (convert text to numbers) and the cleansing of noisy characters, we may also stem our words. Stemming is the process of reducing a word to its root form. For example, the words “running”, “runs”, “ran” and “run” would be reduced to “run”. This is useful as it reduces the number of features that our model needs to learn, and thus it also helps to reduce the noise in our data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="https://devopedia.org/images/article/218/8583.1569386710-1400.webp"/> <img src="https://devopedia.org/images/article/218/8583.1569386710.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In a nutshell, our pipeline will look like this:</p> <ol> <li>Lowering the text, so uppercase and lowercase letters are considered the same and we don’t have to deal with different representations of the same word.</li> <li>Tokenization, using blank spaces as separators we separate our strings into tokens that we may look up in a learnt mapping of <code class="language-plaintext highlighter-rouge">{string: integer}</code> often called <em>vocabulary</em>.</li> <li>Removing punctuation and stopwords, to reduce the noise.</li> <li>Stemming the words to their root form thanks to the <a href="https://tartarus.org/martin/PorterStemmer/">Porter Stemmer</a>, to reduce the number of features.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span>
    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">stopwords</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># Lowercase text
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>

    <span class="c1"># Tokenize text
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>

    <span class="c1"># Remove punctuation
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">]</span>

    <span class="c1"># Remove stopwords
</span>    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

    <span class="c1"># Stem text
</span>    <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
    <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">preprocessed_text</span><span class="p">]</span>

    <span class="k">return</span> <span class="s">' '</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">preprocessed_text</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h2 id="modelling">Modelling</h2> <h3 id="logistic-regression">Logistic Regression</h3> <p>The first step in implementing this method is to create a frequency table of the appearance of each token in the different classes. In this case, we will have two records for each token: one for the positive class and one for the negative class. This table will be used to calculate the probability of a class given a token, that we can later on aggregate to the probability of a class given a set of tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_word_dict</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
            <span class="c1"># define the key, which is the word and label tuple
</span>            <span class="n">pair</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
            
            <span class="c1"># if the key exists in the dictionary, increment the count
</span>            <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
                <span class="n">result</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># else, if the key is new, add it to the dictionary and set the count to 1
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">result</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>After creating the frequency table, we can then transform our sentences into a matrix of $N*K$ dimensions, where $N$ is the number of sentences and $K$ is the number of classes in our problem, which will represent the sum of appearances of each token in each class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">freqs</span><span class="p">,</span> <span class="n">preprocess</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">preprocess</span><span class="p">:</span>
      <span class="n">word_l</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">stopwords</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">word_l</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="c1">#bias term is set to 1
</span>    <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> 
    
    <span class="c1"># loop through each token
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)]</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">preprocessed_train</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">preprocessed_train</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">preprocessed_test</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">preprocessed_test</span><span class="p">.</span><span class="n">values</span><span class="p">):</span>
    <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
</code></pre></div></div> <p>Now we just have to fit our <code class="language-plaintext highlighter-rouge">LogisticRegression</code> model, which will learn a bias parameter (depending on the proportion of each class in the train corpus) and the weights of each feature (the sum of appearances of each token in each class). For this model we will use the <code class="language-plaintext highlighter-rouge">sklearn</code> implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">log_model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">log_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <h3 id="naive-bayes">Naive Bayes</h3> <p>As a second traditional method, we select <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a>. This method is based on the conditional probability of an example to belong to a certain class given the probabilities of each of the tokens, which will be summarized in what’s called <em>loglikelihoods</em>. To calculate this probability, we will be using the same frequency table that we calculated for the Logistic Regression:</p> \[P(W_{pos}) = \frac{freq_{pos} + 1}{N_{pos} + V}\] \[P(W_{neg}) = \frac{freq_{neg} + 1}{N_{neg} + V}\] \[\text{loglikelihood} = \log \left(\frac{P(W_{pos})}{P(W_{neg})} \right)\] <p>Besides, a bias term is fitted which represents the overall likelihood of an example of a certain class to be draft from the corpus. This bias term is called <em>logprior</em>.</p> \[\text{logprior} = \log (D_{pos}) - \log (D_{neg})\] <p>Finally, after the calculation of these both terms we just need to add them to form the final probability of an example to belong to a particular class, and then perform a <em>softmax</em> operation to retrieve the index with the higher probability. As in this case the problem is binary, we can summarize the equation as:</p> \[p = logprior + \sum_i^N (loglikelihood_i)\] <p>Although there are several implementations of Naive Bayes in Python, given the relative simpleness of this case we can create our own functions in order to get a better grasp of the inner workings of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">):</span>
    <span class="n">loglikelihood</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()}</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># calculate N_pos, N_neg, V_pos, V_neg
</span>    <span class="n">N_pos</span> <span class="o">=</span> <span class="n">N_neg</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="c1"># if the label is positive (greater than zero)
</span>        <span class="k">if</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">N_pos</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
        <span class="c1"># else, the label is negative
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">N_neg</span> <span class="o">+=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span>
    
    <span class="n">D</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
    <span class="n">D_pos</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">D_neg</span> <span class="o">=</span> <span class="n">train_y</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Calculate logprior
</span>    <span class="n">logprior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">D_pos</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">D_neg</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
        <span class="c1"># get the positive and negative frequency of the word
</span>        <span class="n">freq_pos</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span> <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">freq_neg</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span> <span class="nf">if </span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="c1"># calculate the probability that each word is positive, and negative
</span>        <span class="n">p_w_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_pos</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">p_w_neg</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_neg</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_neg</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="c1"># calculate the log likelihood of the word
</span>        <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p_w_pos</span> <span class="o">/</span> <span class="n">p_w_neg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span>


<span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span> <span class="o">=</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">preprocessed_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <p>Once we get our parameters <code class="language-plaintext highlighter-rouge">logprior</code> and <code class="language-plaintext highlighter-rouge">loglikelihood</code>, that represent the bias and the weights of this model, we can use them to predict the class of a new example by aggregating the scores of their tokens:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
    <span class="n">word_l</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># start by adding our logprior
</span>    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">logprior</span>

    <span class="c1"># words that are not in vocabulary simply get ignored
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">loglikelihood</span><span class="p">:</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">p</span>


<span class="k">def</span> <span class="nf">test_naive_bayes</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">,</span> <span class="n">naive_bayes_predict</span><span class="o">=</span><span class="n">naive_bayes_predict</span><span class="p">):</span>
    <span class="n">y_hats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">test_x</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">y_hats</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_hat_i</span><span class="p">)</span>

    <span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">test_y</span> <span class="o">-</span> <span class="n">y_hats</span><span class="p">))</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">error</span>
    <span class="k">return</span> <span class="n">y_hats</span><span class="p">,</span> <span class="n">accuracy</span>
</code></pre></div></div> <p>As we can see, this approach is very similar than the one taken in the Logistic Regression, but it is often more efficient in terms of computational cost and in model performance. In fact, in the case of the IMDb dataset, the Naive Bayes model achieves a better accuracy than even a more complex model based on RNNs (<a href="https://github.com/josumsc/classic-ml-sentiment-analysis/blob/master/src/IMDB_Sentiment_Analysis.ipynb">reference</a>).</p> <h2 id="conclusion">Conclusion</h2> <p>As we just saw, the implementation of a simple model based on the frequency of the tokens in the corpus can be easily explained, implemented and trained. Furthermore, their efficiency allow us to fit these models to a large vocabulary of custom tokens to adapt them to our specific needs without the need of a large amount of data.</p> <p>We should not be narrow-minded and think that due to the emergence of far more powerful algorithms these models are not useful anymore, as they both serve as a good starting point to understand our datasets and as a baseline to compare the performance of more complex models.</p>]]></content><author><name></name></author><category term="nlp"/><category term="python"/><summary type="html"><![CDATA[Using statistical methods to perform sentiment analysis on a dataset of reviews might be simpler than you think.]]></summary></entry><entry><title type="html">Streamlit your Data Science application into AWS</title><link href="https://josumsc.github.io/blog/2023/streamlit-your-application-into-aws/" rel="alternate" type="text/html" title="Streamlit your Data Science application into AWS"/><published>2023-01-14T01:00:00+00:00</published><updated>2023-01-14T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2023/streamlit-your-application-into-aws</id><content type="html" xml:base="https://josumsc.github.io/blog/2023/streamlit-your-application-into-aws/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-01-1400.webp"/> <img src="/assets/img/streamlit-aws-01.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p>To check a more advanced real world application developed using these guidelines please consider visiting <a href="https://github.com/josumsc/twitter-sentiment-analysis">this repository</a></p> </blockquote> <p>Have you ever seen those fancy Data Science web apps that include interactive plots, Machine Learning inference in realtime and custom user inputs? Well, I have and I was always wondering how they were built. I mean, I’m sure that a whole team of front-end engineers, data engineers and data scientists could create something like that, but I was always looking for a simpler solution. And then I found <a href="https://docs.streamlit.io">Streamlit</a>.</p> <h2 id="what-is-streamlit">What is Streamlit?</h2> <p>Streamlit is a Python framework that allows you to deploy Data Science applications in the form of Python scripts, dealing with the back and most of the front-end nuances for you. It’s a great tool for Data Scientists that want to share their work with the world, but don’t want to spend too much time dealing with web development.</p> <p>It also includes a Cloud service to host your applications, which is great for a personal project or a small team. But what if you want to deploy your application to a bigger audience or you are concerned about privacy and security issues? Well, you can do that too, but you’ll need to use AWS, which we will talk about later.</p> <h2 id="how-does-it-work">How does it work?</h2> <p>Streamlit is a Python framework, so you’ll need to install it in your environment. You can do that by running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>streamlit
</code></pre></div></div> <p>If we run <code class="language-plaintext highlighter-rouge">streamlit hello</code> we will be able to see how we have a running web application on our localhost, and with 0 code! Sure enough, our custom Data Science applications will not be so easy to create or simple to use, but the key elements are there.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-02-1400.webp"/> <img src="/assets/img/streamlit-aws-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="creating-a-streamlit-application">Creating a Streamlit application</h2> <p>There are several commands that we can use to create a Streamlit application, being the most important ones explained in the <a href="https://docs.streamlit.io/library/cheatsheet">Cheat Sheet</a>. As we will progress building our app, I’ll leave some comments in the code to explain what is going on:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">streamlit</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="c1"># Sets the page layout to wide and creates a title
</span><span class="n">st</span><span class="p">.</span><span class="nf">set_page_config</span><span class="p">(</span><span class="s">"My dummy Streamlit App"</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="s">"wide"</span><span class="p">)</span>
<span class="c1"># Creates a h1 title
</span><span class="n">st</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="s">"My first Streamlit application"</span><span class="p">)</span>

<span class="c1"># Creates a DataFrame
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="s">'Seller'</span><span class="p">:</span> <span class="p">[</span><span class="s">"A"</span><span class="p">,</span> <span class="s">"B"</span><span class="p">,</span> <span class="s">"C"</span><span class="p">,</span> <span class="s">"D"</span><span class="p">],</span>
    <span class="s">'Sales'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="p">})</span>

<span class="c1"># Creates a sidebar with a multiselect widget
</span><span class="n">sellers_to_filter</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="n">sidebar</span><span class="p">.</span><span class="nf">multiselect</span><span class="p">(</span>
    <span class="s">"Select sellers to filterby"</span><span class="p">,</span>
    <span class="p">[</span><span class="n">seller</span> <span class="k">for</span> <span class="n">seller</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s">"Seller"</span><span class="p">].</span><span class="nf">unique</span><span class="p">()],</span>
    <span class="p">[</span><span class="n">seller</span> <span class="k">for</span> <span class="n">seller</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s">"Seller"</span><span class="p">].</span><span class="nf">unique</span><span class="p">()],</span>
<span class="p">)</span>
<span class="n">filtered_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">"Seller"</span><span class="p">].</span><span class="nf">isin</span><span class="p">(</span><span class="n">sellers_to_filter</span><span class="p">)]</span>

<span class="c1"># Creates 2 columns for plots
</span><span class="n">col1</span><span class="p">,</span> <span class="n">col2</span> <span class="o">=</span> <span class="n">st</span><span class="p">.</span><span class="nf">columns</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Creates a bar chart
</span><span class="n">col1</span><span class="p">.</span><span class="nf">plotly_chart</span><span class="p">(</span><span class="n">px</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">filtered_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">"Seller"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"Sales"</span><span class="p">))</span>

<span class="c1"># Creates a pie chart
</span><span class="n">col2</span><span class="p">.</span><span class="nf">plotly_chart</span><span class="p">(</span><span class="n">px</span><span class="p">.</span><span class="nf">pie</span><span class="p">(</span><span class="n">filtered_df</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s">"Sales"</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="s">"Seller"</span><span class="p">).</span><span class="nf">update_layout</span><span class="p">(</span><span class="n">showlegend</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>

<span class="c1"># Displays the DataFrame as a table
</span><span class="n">st</span><span class="p">.</span><span class="nf">table</span><span class="p">(</span><span class="n">filtered_df</span><span class="p">)</span>
</code></pre></div></div> <p>While we iterate through our perfect web app, we can use the following command to see the changes in real time:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run app.py
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-03-1400.webp"/> <img src="/assets/img/streamlit-aws-03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Our Streamlit application" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="containerizing-our-application">Containerizing our application</h2> <p>Now that our app is complete and we are happy with the results of this first iteration, we can start to think about how to deploy it. As we mentioned before, we could use <a href="https://streamlit.io/cloud">Streamlit Cloud</a> to host our application, but we will use AWS to have more control over our application and to be able to scale it in the future.</p> <p>Although we could deploy our application as a script directly in an EC2 instance, it’s advised to containerized it. This way, we can easily deploy it in any environment and we can also scale up to Kubernetes if needed, for example, or collaborate easier with other teams.</p> <p>To containerize our application, we will use Docker. We will <a href="https://docs.docker.com/get-docker/">install Docker</a> and create a <code class="language-plaintext highlighter-rouge">Dockerfile</code> with the following content:</p> <div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.9</span>

<span class="k">EXPOSE</span><span class="s"> 8501</span>

<span class="k">WORKDIR</span><span class="s"> /app</span>

<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="se">\
</span>    build-essential <span class="se">\
</span>    software-properties-common <span class="se">\
</span>    git

<span class="k">COPY</span><span class="s"> ./requirements.txt /app/requirements.txt</span>

<span class="k">RUN </span>pip3 <span class="nb">install</span> <span class="nt">--upgrade</span> pip <span class="o">&amp;&amp;</span> pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div> <p>And we will also create a <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file with the following content to manage our image and prepare it to include further services if needed:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.9"</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">my-app</span><span class="pi">:</span>
    <span class="na">build</span><span class="pi">:</span>
      <span class="na">context</span><span class="pi">:</span> <span class="s">.</span>
      <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">./Dockerfile</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">$HOME/.aws/config:/root/.aws/config</span>
      <span class="pi">-</span> <span class="s">$HOME/.aws/credentials:/root/.aws/credentials</span>
    <span class="na">command</span><span class="pi">:</span> <span class="s">streamlit run app.py --server.port=8501 --server.address=0.0.0.0</span>
    <span class="na">restart</span><span class="pi">:</span> <span class="s">always</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">8501:8501</span>
</code></pre></div></div> <p>We can test our application in local using the following command on the root of our project:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up
</code></pre></div></div> <p>This should have opened a port 8501 in our localhost, where we can see our application running.</p> <h2 id="deploying-our-application">Deploying our application</h2> <h3 id="aws-configuration">AWS Configuration</h3> <p>In order to deploy our application in AWS, we first need to create an AWS account. If you don’t have one, you can create one <a href="https://aws.amazon.com/">here</a>. Please consider that we will only be using the free tier offered by Amazon (<a href="aws.amazon.com/free">more info</a>) to avoid extra costs with our examples. To create an alert in case you surpass the free tier usage go to Account Settings &gt; Budgets &gt; Create Budget and select the <code class="language-plaintext highlighter-rouge">Zero spend budget</code> template:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-04-1400.webp"/> <img src="/assets/img/streamlit-aws-04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Create Budget Alert" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Then we can create a Group and a IAM user following the instructions <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html">here</a>, downloading the secret key and the access key. We will need these credentials to connect to our AWS account programmatically.</p> <p>Once you have your account, you can create an EC2 instance. We recommend using <strong>t2.micro</strong> with Amazon Linux for testing due to the low cost, but you can use a bigger instance if you want to scale up your application.</p> <p>On the Security Group configuration, we need to expose the port that’s going to be used for our application, which in this case will be the 8501. We also need to add a rule to allow SSH access to our instance, so we can connect to the instance later. On this step, it’s also important to create a key pair to authenticate our connection while connecting to the instance via SSH.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/streamlit-aws-05-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/streamlit-aws-05-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/streamlit-aws-05-1400.webp"/> <img src="/assets/img/streamlit-aws-05.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Security Group Configuration" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Once our EC2 is configured, feel free to run it using the AWS console so we can properly used it.</p> <h3 id="connecting-to-the-ec2-instance">Connecting to the EC2 instance</h3> <p>Now that our EC2 instance is running, we can connect to it using SSH. We can do that by running the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-i</span> &lt;path-to-key-pair&gt; ec2-user@&lt;public-ip&gt;
</code></pre></div></div> <h3 id="installing-docker-in-ec2">Installing Docker in EC2</h3> <p>Once we are connected to our EC2 instance, we need to install Docker. We can do that by running the following commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum update <span class="nt">-y</span>
<span class="nb">sudo </span>amazon-linux-extras <span class="nb">install </span>docker
<span class="nb">sudo </span>service docker start
<span class="nb">sudo </span>usermod <span class="nt">-a</span> <span class="nt">-G</span> docker ec2-user
</code></pre></div></div> <p>And now we just need to download the code from our repository and build the image:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone &lt;git-repository-url&gt;
<span class="nb">cd</span> &lt;project-name&gt;
docker-compose up
</code></pre></div></div> <h3 id="adding-security-to-our-application">Adding security to our application</h3> <p>Streamlit can be integrated with SSOs such as <a href="https://www.okta.com">OKTA</a> and it’s the recommended option for production environments. However, it also offer different alternatives for use cases without SSO.</p> <p>The first alternative would be to use the <code class="language-plaintext highlighter-rouge">.streamlit/secrets.toml</code> file to store our credentials and later on check for the contents of this file in our application. An example of this integration, following the code in the <a href="https://docs.streamlit.io/knowledge-base/deploy/authentication-without-sso">Streamlit guidelines</a>, could be the following:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># .streamlit/secrets.toml</span>

<span class="o">[</span>passwords]
<span class="c"># Follow the rule: username = "password"</span>
alice_foo <span class="o">=</span> <span class="s2">"streamlit123"</span>
bob_bar <span class="o">=</span> <span class="s2">"mycrazypw"</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># app.py
</span><span class="kn">import</span> <span class="n">streamlit</span> <span class="k">as</span> <span class="n">st</span>

<span class="k">def</span> <span class="nf">check_password</span><span class="p">():</span>
    <span class="s">"""Returns `True` if the user had a correct password."""</span>

    <span class="k">def</span> <span class="nf">password_entered</span><span class="p">():</span>
        <span class="s">"""Checks whether a password entered by the user is correct."""</span>
        <span class="nf">if </span><span class="p">(</span>
            <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"username"</span><span class="p">]</span> <span class="ow">in</span> <span class="n">st</span><span class="p">.</span><span class="n">secrets</span><span class="p">[</span><span class="s">"passwords"</span><span class="p">]</span>
            <span class="ow">and</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password"</span><span class="p">]</span>
            <span class="o">==</span> <span class="n">st</span><span class="p">.</span><span class="n">secrets</span><span class="p">[</span><span class="s">"passwords"</span><span class="p">][</span><span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"username"</span><span class="p">]]</span>
        <span class="p">):</span>
            <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password_correct"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">del</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password"</span><span class="p">]</span>  <span class="c1"># don't store username + password
</span>            <span class="k">del</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"username"</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password_correct"</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">if</span> <span class="s">"password_correct"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">:</span>
        <span class="c1"># First run, show inputs for username + password.
</span>        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span><span class="s">"Username"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"username"</span><span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span>
            <span class="s">"Password"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">"password"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"password"</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"password_correct"</span><span class="p">]:</span>
        <span class="c1"># Password not correct, show input + error.
</span>        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span><span class="s">"Username"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"username"</span><span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="nf">text_input</span><span class="p">(</span>
            <span class="s">"Password"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">"password"</span><span class="p">,</span> <span class="n">on_change</span><span class="o">=</span><span class="n">password_entered</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s">"password"</span>
        <span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="s">"😕 User not known or password incorrect"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Password correct.
</span>        <span class="k">return</span> <span class="bp">True</span>

<span class="k">if</span> <span class="nf">check_password</span><span class="p">():</span>
    <span class="c1"># Here comes the main content of our application
</span></code></pre></div></div> <p>Even though we can use this approach, it’s not recommended to store our credentials in plain text. We can use <a href="https://aws.amazon.com/secrets-manager/">AWS Secrets Manager</a> to store our credentials and retrieve them in our application. This way, we can avoid storing our credentials in plain text and we can also use the same credentials for different applications. This approach is a little bit more complex, but I find it to be much more secure.</p> <p>After registering our secret in AWS Secrets Manager, we can retrieve and store it in our application session state by using code like the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ast</span>
<span class="kn">import</span> <span class="n">boto3</span>
<span class="kn">import</span> <span class="n">botocore</span>

<span class="k">def</span> <span class="nf">state_password_dict</span><span class="p">(</span><span class="n">secret_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">region_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">boto3</span><span class="p">.</span><span class="n">session</span><span class="p">.</span><span class="nc">Session</span><span class="p">()</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">client</span><span class="p">(</span><span class="n">service_name</span><span class="o">=</span><span class="s">"secretsmanager"</span><span class="p">,</span> <span class="n">region_name</span><span class="o">=</span><span class="n">region_name</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">get_secret_value_response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get_secret_value</span><span class="p">(</span><span class="n">SecretId</span><span class="o">=</span><span class="n">secret_name</span><span class="p">)</span>
        <span class="n">secret_string</span> <span class="o">=</span> <span class="n">get_secret_value_response</span><span class="p">[</span><span class="s">"SecretString"</span><span class="p">]</span>
        <span class="n">secret_dict</span> <span class="o">=</span> <span class="n">ast</span><span class="p">.</span><span class="nf">literal_eval</span><span class="p">(</span><span class="n">secret_string</span><span class="p">)</span>
        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"secret_dict"</span><span class="p">]</span> <span class="o">=</span> <span class="n">secret_dict</span>
    <span class="k">except</span> <span class="n">botocore</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="n">ClientError</span><span class="p">:</span>
        <span class="c1"># if secret not found then create empty dict
</span>        <span class="n">st</span><span class="p">.</span><span class="n">session_state</span><span class="p">[</span><span class="s">"secret_dict"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre></div></div> <p>Later on, instead of checking the credentials given by the user against those in <code class="language-plaintext highlighter-rouge">st.secrets</code>, we can check them against the credentials stored in our session state by using <code class="language-plaintext highlighter-rouge">st.session_state["secret_dict"]</code> instead.</p> <blockquote> <p>Consider that in order to use this code we need to have in our EC2 instance the AWS config files and add them to our Docker image. You can find more information about this in the <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html">AWS documentation</a>. Go ahead and check our <code class="language-plaintext highlighter-rouge">docker-compose.yml</code> file to see how we did it.</p> </blockquote> <h2 id="final-remarks">Final remarks</h2> <p>Even though this guide was more focused on AWS, I hope it has illustrated how to easily deploy a ML application in the cloud. No matter which service or framework you use, at the end of the day the core concepts such as containerization and deployment are very similar regardless of our vendor choices.</p> <p>It’s true that Streamlit provide us a friendly framework to create our applications, but it’s also true that we need to be aware of the security and privacy issues that we are facing when deploying our code in the cloud. Some DevOps knowledge always come in handy when dealing with these issues, alongside with a good understanding of the cloud services that we are using.</p> <p>I hope that this guide has helped you to understand how to deploy your ML application in the cloud, and we hope to see you soon in our next post!</p>]]></content><author><name></name></author><category term="mlops"/><category term="aws"/><category term="python"/><summary type="html"><![CDATA[Deploy your Streamlit application into AWS and create easily a Data Science web app]]></summary></entry><entry><title type="html">Dealing with Imbalanced Datasets</title><link href="https://josumsc.github.io/blog/2023/dealing-with-imbalanced-datasets/" rel="alternate" type="text/html" title="Dealing with Imbalanced Datasets"/><published>2023-01-08T01:00:00+00:00</published><updated>2023-01-08T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2023/dealing-with-imbalanced-datasets</id><content type="html" xml:base="https://josumsc.github.io/blog/2023/dealing-with-imbalanced-datasets/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imbalanced-datasets-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imbalanced-datasets-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imbalanced-datasets-01-1400.webp"/> <img src="/assets/img/imbalanced-datasets-01.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/credit-card-fraud-detection">This repository</a> serves as an illustration of how this problem may appear in a real-world scenario and how to deal with it.</p> </blockquote> <p>Imagine the CFO of your organization comes to you complaining about how the recent uprise of e-commerce after the Covid outbreak has increased the risk of credit card fraud. As a Data Scientist, you start gathering data from the company’s financial records and start building a baseline model that can show the potential of this task. After a couple of days of work, you have a model that predicts fraudulent transactions with 99.9% precision. You run to the CFO to inform her/him of the good news but you get reprimanded as your model is deemed useless. What happened?</p> <p>The problem is that, as the dataset is highly imbalanced, you model is predicting that every one of the transactions is legit, so it’s unable to detect one single fraudulent transaction. This is a common problem in many industries, and it’s important to know how to deal with it.</p> <h2 id="what-is-an-imbalanced-dataset">What is an imbalanced dataset?</h2> <p>An imbalanced dataset is a dataset where the number of observations in one class is significantly higher than the number of observations in the other classes. In the example above, the number of fraudulent transactions is much lower than the number of legit transactions, with a proportion of 1 fraudulent transaction by at least 999 legit ones. This is a common problem in many industries, such as fraud detection, medical diagnosis, and mechanical engineering, where defective parts are much rare than correct ones.</p> <p>Imbalanced datasets cause problems in Machine Learning as models are focused on maximizing the accuracy of the ensemble of predictions. In the example above, the model will predict that every transaction is legit, and it will be right 99.9% of the time. This is a problem because the model will not able to detect any of the fraudulent transactions, proving itself useless for the task at hand.</p> <h2 id="how-to-deal-with-imbalanced-datasets">How to deal with imbalanced datasets</h2> <p>In the academic literature several methods are approached to deal with this particular problem. As with any other problem in Machine Learning, there is no such thing as a free lunch, and each method has its own pros and cons. In this post, I will cover the most common methods to deal with imbalanced datasets, and I will show how to implement them in Python.</p> <h3 id="dataset-resampling">Dataset resampling</h3> <p>One of the main ways to deal with imbalanced datasets is to directly resample them. The 2 main ways to perform this operation are Oversampling, which consists in adding more observations to the minority class, and Undersampling, which consists in removing observations from the majority class. In the example above, we could add more fraudulent transactions to the dataset, or we could remove legit transactions from the dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imbalanced-datasets-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imbalanced-datasets-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imbalanced-datasets-02-1400.webp"/> <img src="/assets/img/imbalanced-datasets-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Although we could use random approaches to perform both over and under sampling, to perform this operations we have more advanced methods such as <a href="https://arxiv.org/abs/1106.1813">SMOTE</a> or <a href="https://www.sciencedirect.com/science/article/pii/S0957417422005280">NearMiss</a>, both based on Nearest Neighbors models, for repectively over and under sampling. These methods are implemented in the <a href="https://imbalanced-learn.org/stable/">imbalanced-learn</a> Python library are easy enough to implement:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">imblearn.under_sampling</span> <span class="kn">import</span> <span class="n">NearMiss</span>
<span class="kn">from</span> <span class="n">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>

<span class="c1"># Change between the desired resampling method
</span><span class="n">resampler</span> <span class="o">=</span> <span class="nc">NearMiss</span><span class="p">()</span>
<span class="c1"># resampler = SMOTE()
</span>
<span class="n">X_resampled</span><span class="p">,</span> <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">resampler</span><span class="p">.</span><span class="nf">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <p>This kind of transformers work quite fine out-of-the-box, but some parameters we may want to tune are the number of neighbors to consider when resampling, and the sampling proportions. In the example above, we are using the default parameters, but we could tune them to get better results.</p> <blockquote> <p>It’s extremely important not to resample the test partition of the dataset, as we want it to be as close as possible to the real world scenario, when the classes will keep their natural imbalance.</p> </blockquote> <h3 id="metric-selection">Metric Selection</h3> <p>Another way to deal with imbalanced datasets is to select a metric that is more focused on the minority class. In the example above, we could use the <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a> instead of the accuracy, as it is more focused on the minority class. This way, we will be able to detect the fraudulent transactions, even if we are not able to detect all of them.</p> <p>As with many other metrics, the F1 score is implemented in the <a href="https://scikit-learn.org/stable/">scikit-learn</a> library:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">],</span>
                            <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"F1 obtained: </span><span class="si">{</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'macro'</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>If we are instead using PyTorch, we can use the <a href="https://torchmetrics.readthedocs.io/en/stable/classification/f1_score.html">F1 score</a> implemented in the <a href="https://torchmetrics.readthedocs.io/en/latest/">torchmetrics</a> library:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torchmetrics</span> <span class="kn">import</span> <span class="n">F1Score</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Simple neural network with 2 hidden layers
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                            <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                            <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">],</span>
                            <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">f1</span> <span class="o">=</span> <span class="nc">F1Score</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s">"multiclass"</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s">'macro'</span><span class="p">)</span>

<span class="k">with</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="k">as</span> <span class="n">pbar</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="n">pbar</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pbar</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> - F1: </span><span class="si">{</span><span class="nf">f1</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"F1 obtained: </span><span class="si">{</span><span class="nf">f1</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>Some other useful metrics to consider here are the <a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision-Recall curve</a> and the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a>, so feel free to check them out while building and evaluating your models.</p> <h3 id="cost-sensitive-learning">Cost-sensitive learning</h3> <p>Another way to deal with imbalanced datasets is to use cost-sensitive learning. In this approach, we assign a cost to each class, and we use this cost to weight the loss function of the model. This way, the model will be more focused on minimizing the loss of the minority class, and it will be less focused on minimizing the loss of the majority class. This approach is implemented in the <a href="https://scikit-learn.org/stable/">scikit-learn</a> library with the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">class_weight</a> parameter:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The code would be the same as in the previous section but with the following change
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mf">0.94</span><span class="p">})</span>
</code></pre></div></div> <p>A similar implementation can take place in PyTorch with the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">torch.nn.CrossEntropyLoss</a> class:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The code would be the same as in the previous section but with the following change
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.94</span><span class="p">]))</span>
</code></pre></div></div> <h3 id="do-nothing">Do nothing</h3> <p>Yes! You read that right. Sometimes, the best way to deal with imbalanced datasets is to do nothing. In some cases, the model will be able to detect the minority class even if it is not able to detect all of them. With neural networks this approach has gained some popularity recently due to their predictive power, so do not be afraid to try it out.</p> <h2 id="final-remarks">Final remarks</h2> <p>Imbalanced datasets are one of the most common problems on modern applications of Machine Learning. It’s difficult to stumble upon a dataset that is perfectly balanced in a production environment. In this article, we have seen some of the most common approaches to deal with imbalance, and we have seen how to implement them in Python. The key takeaway from this article is that there is no one-size-fits-all solution to this problem. You will have to validate several approaches with the test set against imbalance robust metrics such as F1 or AUC.</p> <p>I hope that you have found this article useful, and that you will be able to use these techniques in your future projects. See you soon!</p>]]></content><author><name></name></author><category term="data preparation"/><category term="pytorch"/><category term="python"/><summary type="html"><![CDATA[Imbalanced datasets can lead to 99% precision and 0% predictive power, how could we then fit our models?]]></summary></entry><entry><title type="html">Explicit and Implicit Collaborative Filtering</title><link href="https://josumsc.github.io/blog/2022/collaborative-filtering/" rel="alternate" type="text/html" title="Explicit and Implicit Collaborative Filtering"/><published>2022-07-30T01:00:00+00:00</published><updated>2022-07-30T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2022/collaborative-filtering</id><content type="html" xml:base="https://josumsc.github.io/blog/2022/collaborative-filtering/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/collaborative-filtering-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/collaborative-filtering-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/collaborative-filtering-01-1400.webp"/> <img src="/assets/img/collaborative-filtering-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>There are several situations when we may want to recommend a product or a piece of content to a user in order to increase our revenue, the engagement of the user with our site or the value perceived by the user while experiencing our site. We cannot recommend items that the user has already left a review on, as he or she has already seen the product and is well-aware of its features. Therefore, we must adventure and try to estimate which items of our catalog are the ones that the user might be interested in, without having an explicit signal for that particular item/user combination.</p> <p>This is where Collaborative Filtering (CF) comes in. Collaborative Filtering is a technique that allows us to estimate the interest of a user to a set of items. The idea is that we can use the reviews of other users to estimate the interest of a different user on the items that those first users reviewed. It is similar to say:</p> <blockquote> <p>“Person A liked products X, Y and Z. Person B liked products X and Z. Therefore, B might like Y with a reasonable confidence.”.</p> </blockquote> <p>This interesting and intuitive process is done by assigning $K$ latent features for each item, and then infer for both the users and the items each of these features so we can see how the user and the item are related. These latent factors can be imagined as the implicit features that our items have, such as the “recentness” of an item in the market, how eco-friendly it is or how related is to a particular topic, such as culture or health, but in the end they are just vectors that are optimized to minimize a cost function, similarly as it happens with <em>word embeddings</em> in NLP tasks.</p> <p>In order to make these predictions and approximate the users preferences successfully we must check the data available and thus the restrictions of the models we may develop to help us with the task at hand. Depending on the data we have, there are two main branches of collaborative filtering models:</p> <ol> <li><strong>Explicit Collaborative Filtering</strong>, which are the most common models, and the ones that are ancient and proven by the research community. To apply them we need an abundance of explicit scores on the user/item combinations, such a rating from 1 to 5 stars or a good/bad review.</li> <li><strong>Implicit Collaborative Filtering</strong>, which are models trained without any explicit score at a user/item level. To apply them we use the signals left by the user on our sites, such as the item page views, the add-to-cart events or the purchases of the user.</li> </ol> <h2 id="explicit-collaborative-filtering">Explicit Collaborative Filtering</h2> <blockquote> <p>For an example of Explicit Collaborative Filtering please check this repository: <a href="https://github.com/josumsc/movielens-recommender">MovieLens Recommender</a></p> </blockquote> <p>As we mentioned before, the explicit collaborative filtering models are the ones that try to predict a known variable, which could be a continuous value as a rating from 1 to 5 stars or a categorical value as a good/bad review. Thus, we face a supervised learning problem, given a matrix X of user/item combinations and a vector y of explicit scores, and depending on the form of the scores we will be using a regressor or a classifier. Now that we know the kind of problem we are facing, it is simpler to come up with good solutions to solve it, so let’s explore a few.</p> <h3 id="classic-collaborative-filtering">Classic Collaborative Filtering</h3> <p>A few years back, when Deep Learning was not still as popular as it is and computational resources were scarce, we had to use a very simple model to predict the ratings of our users. Although, this primordial model was not bad at all, and it served as foundation to the vast majority of the newer models. This model was simply called <strong>Collaborative Filtering</strong> and it was based on the random assignment of $K$ latent factors for the matrices of users and items, and then use those matrices to infer the ratings of the users to the items by multiplying them. After that, we could compute our loss depending on the kind of label we have (either <em>MSE</em> for regression or <em>crossentropy</em> for classification, for example) and if the score was off then we could use an optimizer such as <em>SGD</em> to adjust the latent factors to the correct value. Then, we would compute again the ratings of the users to the items, finishing our training cycle.</p> <blockquote> <p>If we want a more in depth view of classic collaborative filtering, <a href="https://www.coursera.org/learn/machine-learning-course/lecture/2WoBV/collaborative-filtering">this video by Andrew Ng</a> is an invaluable resource.</p> </blockquote> <p>We can apply this method using simple <code class="language-plaintext highlighter-rouge">NumPy</code>, but as it is based on linear algebra and we might want to take advantage of our GPUs let’s see an example using <code class="language-plaintext highlighter-rouge">PyTorch</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Recommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">y_range</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Recommender</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">y_range</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">users</span> <span class="o">*</span> <span class="n">movies</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">result</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_bias</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>As we can see, we instantiate both the weights and biases for each user and item and then simply multiply their weight matrices together. We also add the bias to the result of the multiplication, and we clip the result to the range of the ratings we have in order to make our network converge faster. We could instantiate and train the model using this snippet of code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Starting epoch </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss obtained: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">n_users</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">userId</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">n_movies</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">movieId</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">emb_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">y_range</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Recommender</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">y_range</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</code></pre></div></div> <p>Despite its simplicity, this model should be able to predict the ratings of our users with a decent error if we tune the number of latent factors and the learning rate accordingly, although in the end it’s just a linear model and it has its drawbacks and limitations, so let’s see how we can build upon it.</p> <h3 id="deep-collaborative-filtering">Deep Collaborative Filtering</h3> <p>Breakthroughs as the adoption of GPUs by the deep learning community, the availability of immense quantities of data and new parameter initialization techniques or activation functions have awaken a new wave of interest in the field of deep learning and neural networks. This also apply to the field of recommender systems, as thousands of websites can make use of better recommendations for their users.</p> <p>Enter the <strong>feed-forward collaborative filtering models</strong>, which build upon the latent factors of the classic collaborative filtering models by adding several layers on top of them. These layers allow our models to find deeper and more complicated relationships between the users and the items, and thus, to predict the ratings of the users to the items with a better accuracy. It is important to mind that these models are nothing but an application of the ideas previously seen, which are sometimes more than enough to provide good recommendations, and that they come with a higher computational cost, so it may be wise to test first a simpler model and then iterate upon it.</p> <p>Continuing our PyTorch code, we now will see how to build this kind of models:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FFRecommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">y_range</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">FFRecommender</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">user_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">movie_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">emb_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_units</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">n_units</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_range</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">y_range</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">users</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">user_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">movies</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">movie_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">users</span><span class="p">,</span> <span class="n">movies</span><span class="p">],</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">y_range</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div> <p>The additions to the code come as linear layers. We also added a dropout layer as regularization and drop the matrix multiplication, as PyTorch’s <code class="language-plaintext highlighter-rouge">nn.Linear</code> does that for us. In this occasion, the code to instantiate and train the model is very similar to the one we used for the classic collaborative filtering model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_units</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">ff_model</span> <span class="o">=</span> <span class="nc">FFRecommender</span><span class="p">(</span><span class="n">n_users</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">y_range</span><span class="p">)</span>
<span class="n">ff_model</span> <span class="o">=</span> <span class="n">ff_model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">ff_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>

<span class="nf">train_model</span><span class="p">(</span><span class="n">ff_model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</code></pre></div></div> <p>These models are expected to reach a better score than the previous ones due to their higher complexity, although it comes at a cost in form of more hyperparameters to tune and higher computational cost.</p> <h3 id="sequential-collaborative-filtering">Sequential Collaborative Filtering</h3> <p>Continuing our journey down the rabbit hole of neural networks, we can now build a <strong>sequential collaborative filtering model</strong>. This model is a combination of the feed-forward and the recurrent models, which allow us to think of user preferences not as a fixed vector at a user level but more as a sequence based tensor that depends on the particularities of the items reviewed and their order. The idea behind this model is to capture the effect of recent reviews of users on the next item to be scored as, for instance, a user may decrease the score of a romance movie if he or she has only been watching this genre of movies lately and grew tired of them.</p> <p>On most of the occasions, our data will be in a tabular form, so we may need first to convert them to sequences to pass through our model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">userId</span> <span class="ow">in</span> <span class="n">df_train</span><span class="p">.</span><span class="n">userId</span><span class="p">.</span><span class="nf">unique</span><span class="p">():</span>
    <span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">'userId'</span><span class="p">]</span> <span class="o">==</span> <span class="n">userId</span><span class="p">].</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s">'timestamp'</span><span class="p">)</span>
    <span class="n">user_sequence</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_filtered</span><span class="p">[</span><span class="s">'movieId'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="n">df_filtered</span><span class="p">[</span><span class="s">'rating'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
    <span class="n">sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">user_sequence</span><span class="p">)</span>

<span class="n">cutoff_index</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">sequences_train</span><span class="p">,</span> <span class="n">sequences_test</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:</span><span class="n">cutoff_index</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[</span><span class="n">cutoff_index</span><span class="p">:]</span>
</code></pre></div></div> <p>Now that we created the sequences to serve to our model, we may use Recurrent Neural Networks (RNNs) to implement this solution. The most common RNNs are LSTMs and GRUs, and due to its higher complexity and popularity we will be using LSTMs here:</p> <blockquote> <p>For a more in depth explanation of RNNs, please refer to <a href="https://www.youtube.com/watch?v=SEnXr6v2ifU">this video by MIT</a>.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SeqRecommender</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_items</span><span class="p">,</span> <span class="n">num_output</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">item_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_items</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_output</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_hidden</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># initialize both hidden layers
</span>        <span class="nf">return </span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)),</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">item_embeddings</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                                        <span class="n">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="n">rating_scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">rating_scores</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sequence</span><span class="p">):</span>
        <span class="n">rating_scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rating_scores</span>
</code></pre></div></div> <p>Regarding the training of our model, we may again see that the code is quite similar:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_units_lstm</span> <span class="o">=</span> <span class="mi">30</span>

<span class="n">seq_model</span> <span class="o">=</span> <span class="nc">SeqRecommender</span><span class="p">(</span><span class="n">emb_size</span><span class="p">,</span> <span class="n">n_units_lstm</span><span class="p">,</span> <span class="n">n_movies</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">seq_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>

<span class="n">seq_model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">25</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">target_ratings</span> <span class="ow">in</span> <span class="n">sequences_train</span><span class="p">:</span>
        <span class="n">seq_model</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">seq_model</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">seq_model</span><span class="p">.</span><span class="nf">init_hidden</span><span class="p">()</span>
        <span class="n">sequence_var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">LongTensor</span><span class="p">(</span><span class="n">sequence</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="s">'int64'</span><span class="p">)))</span>
        <span class="n">ratings_scores</span> <span class="o">=</span> <span class="nf">seq_model</span><span class="p">(</span><span class="n">sequence_var</span><span class="p">)</span>
        <span class="n">target_ratings_var</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">target_ratings</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">ratings_scores</span><span class="p">,</span> <span class="n">target_ratings_var</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss obtained on epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>In case we want to make our model more complex, we may want to add a second layer of LSTM to the model, or a second linear layer after the first one. Nevertheless, I still recommend using first a simpler model, evaluate the results and build add more layers to it if necessary.</p> <h2 id="implicit-collaborative-filtering">Implicit Collaborative Filtering</h2> <blockquote> <p>We can see the code of a practical example on this kind of tasks here: <a href="https://github.com/josumsc/retrocket-implicit-recommender">Retail Rocket Recommender</a></p> </blockquote> <p>The methods for <em>Explicit Collaborative Filtering</em> are straight-forward, effective and widely used by industry peers. Nevertheless, they require many data points of users explicitly informing us of how aligned a particular item is to their interests, which cannot always be obtained easily due to restrictions in our website or legal issues, for instance. We mentioned in the beginning of this article that there are methods to deal with training collaborative filtering models on implicit datasets, and we will see the most renowned one: <strong>Alternating Least Squares</strong>.</p> <p>The Alternating Least Squares (ALS) method was <a href="http://yifanhu.net/PUB/cf.pdf">first reviewed by Yifan Hu et al</a> and describes the use of a <em>confidence</em> matrix composed by the aggregated signal of the events a particular user generated while interacting with a certain product to infer the <em>preference</em> matrix, which is the binary version of the previous matrix and determines if a user has a preference for a particular product or not. Once we have the preference matrix, we compare it with the dot product of the latent factors of our users and items and weight that score by multiplying the confidence matrix, so the combinations with more signal are the ones that add more to the cost. Lastly, we will also be including a regularization parameter $\lambda$ which will help us avoid overfitting using a similar technique to L2 regularization. In this kind of models we have to avoid overfitting as much as possible as it would lead our estimators to only repeat the combinations seen previously in the training data.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/collaborative-filtering-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/collaborative-filtering-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/collaborative-filtering-02-1400.webp"/> <img src="/assets/img/collaborative-filtering-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Using a business case of an e-commerce trying to give their users the most adequate recommendations, we could apply ALS by first computing our <em>confidence</em> matrix, weighting the different signals of the user/item interactions to a single value. We will use for this case the following signals:</p> <ol> <li><strong>Product Page views</strong>, which will score as 1 in our confidence matrix.</li> <li><strong>Add-to-cart events</strong>, which will score as 5.</li> <li><strong>Transaction events</strong>, which will score as 25.</li> </ol> <p>With the difference on scores for each signal we expect to prioritize the strongest events above more frequent ones which may be deceiving, such as the page views. We can use Python to compute this confidence matrix easily given those weights:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'event'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'view'</span> <span class="k">else</span> <span class="mi">5</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'addtocart'</span> <span class="k">else</span> <span class="mi">25</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'transaction'</span> <span class="k">else</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div></div> <p>Later on, we make this matrix sparse, as it’s mostly composed by 0s and this conversion will speed up training and save us a lot of memory space that in cases with thousands of products and clients might be critical:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="s">"category"</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="nf">as_ordered</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="s">"category"</span><span class="p">).</span><span class="n">cat</span><span class="p">.</span><span class="nf">as_ordered</span><span class="p">()</span>

<span class="n">sparse_item_user</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">])))</span>
<span class="n">sparse_user_item</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="nf">csr_matrix</span><span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="s">'event_scores'</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'visitorid'</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">'itemid'</span><span class="p">])))</span>
</code></pre></div></div> <p>Once the matrix is built and on a sparse form, we may simply use the ALS implementation of the library <code class="language-plaintext highlighter-rouge">implicit</code> to infer the latent factors of our items and users.</p> <blockquote> <p>P.S: Although ALS is the most common method for implicit recommendations, the <code class="language-plaintext highlighter-rouge">implicit</code> library has implementations for the rest of algorithms available for this task. If you are curious about them please go ahead and visit <a href="https://implicit.readthedocs.io/en/latest/">its documentation</a>.</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">latent_factors</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">regularization</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">40</span>

<span class="n">conf_matrix</span> <span class="o">=</span> <span class="p">(</span><span class="n">sparse_item_user</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="s">'double'</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">implicit</span><span class="p">.</span><span class="n">als</span><span class="p">.</span><span class="nc">AlternatingLeastSquares</span><span class="p">(</span>
    <span class="n">factors</span><span class="o">=</span><span class="n">latent_factors</span><span class="p">,</span>
    <span class="n">regularization</span><span class="o">=</span><span class="n">regularization</span><span class="p">,</span>
    <span class="n">iterations</span><span class="o">=</span><span class="n">n_iter</span>
<span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>
</code></pre></div></div> <p><em>Et voilà!</em>, our model is trained and ready to make suggestions for our users. Not only that, but we can also see which products are most similar among them, so we can infer substitute items in case of stock outs or to suggest products that solve the same needs but provide us a higher margin. The code for both this use cases would be the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">recommend_item_to_user</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">visitorid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">recommended</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">recommend</span><span class="p">(</span><span class="n">visitorid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">[</span><span class="n">visitorid</span><span class="p">],</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">recommended</span>


<span class="k">def</span> <span class="nf">similar_items_to_item</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">itemid</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">similar</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">similar_items</span><span class="p">(</span><span class="n">itemid</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">similar</span>
</code></pre></div></div> <p>And we can call these functions as we would do with any Python module:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Choose an userid
</span><span class="n">userid</span> <span class="o">=</span> <span class="mi">97154</span>
<span class="n">recommended_items</span> <span class="o">=</span> <span class="nf">recommend_item_to_user</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">userid</span><span class="p">,</span> <span class="n">sparse_item_user</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Recommended items for user </span><span class="si">{</span><span class="n">userid</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">recommended_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Choose an itemid
</span><span class="n">itemid</span> <span class="o">=</span> <span class="mi">350566</span>
<span class="n">similar_items</span> <span class="o">=</span> <span class="nf">similar_items_to_item</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">itemid</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Similar items to </span><span class="si">{</span><span class="n">itemid</span><span class="si">}</span><span class="s">:</span><span class="se">\n</span><span class="si">{</span><span class="n">similar_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusions">Conclusions</h2> <p>In this post we were able to see in detail the main mechanism behind the recommendations we often see on sites as Netflix, Amazon or Alibaba and apply different techniques to increase their capabilities or apply regularization layers as Dropout on them. We also saw how to deal with not having a good enough explicit dataset by using the signals left by our users in our site such as the purchases or the page views to infer their preferences.</p> <p>Hope this was useful and in case you have any doubt about the concepts explained on this article please do not hesitate to connect with me to further discussions. Thank you and keep on learning!</p>]]></content><author><name></name></author><category term="python"/><category term="deep learning"/><category term="pytorch"/><category term="recommender system"/><category term="e-commerce"/><summary type="html"><![CDATA[How can we get to the best recommendation possible when little to no reviews are available?]]></summary></entry><entry><title type="html">Transfer Learning: Standing on the Shoulders of Giants</title><link href="https://josumsc.github.io/blog/2022/transfer-learning/" rel="alternate" type="text/html" title="Transfer Learning: Standing on the Shoulders of Giants"/><published>2022-07-22T01:00:00+00:00</published><updated>2022-07-22T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2022/transfer-learning</id><content type="html" xml:base="https://josumsc.github.io/blog/2022/transfer-learning/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-01-1400.webp"/> <img src="/assets/img/transfer-learning-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p><a href="https://github.com/josumsc/dogs-vs-cats">This repository</a> holds a quick demo of a practical use of the concepts in this article.</p> </blockquote> <p>While trying to solve a machine learning proble, most of us pass through an iterative process in which, after the data collection and analysis tasks, we try different preprocessing and different models to optimize a certain metric, either the crossentropy loss for a classification model or the Huber loss in a regression model, for example.</p> <p>On that process, a <a href="https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765">cycle of underfitting and overfitting</a> is often repeated. In this cycle the first phase is often characterized by our models not being sufficiently complex to capture the different patterns of the data, followed by a phase when we make them too complex and we make them learn specific features of the training set instead of general features and characteristics that could be used in a different dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-02-1400.webp"/> <img src="/assets/img/transfer-learning-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Regularization techniques such as <a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/">Dropout</a> or <a href="https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization">L2 regularization</a> can help us solve the problem of overfitting, as they penalize the complexity of the model. The implementation of these methods is often recommended above the restriction of units per layer or the number of layers, as they are prepared to let those important parameters to compensate for the loss added to the model.</p> <p>Nevertheless, no matter how many regularization we use we eventually end up facing a barrier that we cannot overcome: The scarce volume of data we have. Even if we try to help our models, they only learn what is present in the training set, and if the training set is small or its diversity is not enough our models will eventually fail. In the end, they need to reduce a certain metric as much as possible, and they will adapt to the data whenever possible, so either we make them adapt too much (overfitting) to reduce the training loss, or too little (underfitting) to reduce the discrepancy between the training and validation losses.</p> <p>To help with this problem, we can use <strong>data augmentation</strong>. For example, if we were to use <code class="language-plaintext highlighter-rouge">Keras</code> to train a model, we could use the <code class="language-plaintext highlighter-rouge">ImageDataGenerator</code> to create synthetic data from the training set, and use it to train our model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>

<span class="n">datagen</span> <span class="o">=</span> <span class="nc">ImageDataGenerator</span><span class="p">(</span>
      <span class="n">rotation_range</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
      <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">fill_mode</span><span class="o">=</span><span class="s">'nearest'</span>
<span class="p">)</span>
</code></pre></div></div> <p>Here we can see how using parameters as <code class="language-plaintext highlighter-rouge">horizontal_flip</code> we could create a new image from another one by rotating it over its Y axis, which would imply giving our model an example more for capturing those generic patterns that we commented before.</p> <p>Although, this type of techniques end up using our training set as well, so if this is very poor, we will not be able to solve our problem. In this case, wouldn’t it be great to import a model trained by another person in a better dataset, with a better architecture? Enter Transfer Learning.</p> <h2 id="transfer-learning">Transfer Learning</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-03-1400.webp"/> <img src="/assets/img/transfer-learning-03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>By using public available model repositories such as <a href="https://www.tensorflow.org/hub?hl=es-419">Tensorflow Hub</a> we can access pre-trained models and download them for use in our notebooks. In addition, well known models as GloVe or VGG16 have functions inside their own modules in Keras that we can access in order to load various implementations of those models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.applications</span> <span class="kn">import</span> <span class="n">VGG16</span>

<span class="n">conv_base</span> <span class="o">=</span> <span class="nc">VGG16</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span>
    <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Shows the structure of the convnet
</span><span class="n">conv_base</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

<span class="c1"># Freezing the layers so they don't get modified at training
</span><span class="n">conv_base</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <p>This step load the first layers (often called the <em>body</em> of the network) of the VGG16 model, as the last layers (those usually called the <em>head</em>) are specific for each task, as they are the ones in charge of classifying the input images in the different classes. In this case, adding a classifier over the pretrained model would be easy, and would allow us to train our model together using the same methods we would use with a model conceived completely by us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dropout</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">conv_base</span><span class="p">,</span>
    <span class="nc">Flatten</span><span class="p">(),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">),</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">train_generator</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_generator</span><span class="p">,</span>
    <span class="n">validation_steps</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="fine-tuning">Fine Tuning</h2> <p>The performance of our model at this point would have increased by a large margin, as using a model which already knew the general patterns of the images presented would have saved us tons of time and effort. We were able to tune the model by simply adding a <em>head block</em> on top of the pretrained model, but what happens if we start having more and more data or if the pretrained model does not fit our needs? In those cases we could apply Fine Tuning to further adjust the pretrained model so it can perform better in our particular task.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/transfer-learning-04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/transfer-learning-04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/transfer-learning-04-1400.webp"/> <img src="/assets/img/transfer-learning-04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Fine Tuning is based on the idea of unfreeze the last layers of the model (those which encapsulate more specific patterns and features) and then train our model in our specific problem, so their weights get more adequate to the task at hand. We should consider that this is a risky practice, as if we modify the weights without being sure that the quality of our training set is adequate, we could end up making our model worse or even falling under the <a href="https://towardsdatascience.com/forgetting-in-deep-learning-4672e8843a7f">Catastrophic Forgetting problem</a>. Nevertheless, it is worth a try and very advisable when we know at which extend to use it. As a rule of thumb, avoid modifying the first layers of a pretrained model, and use a small learning rate while fine tuning.</p> <p>Keras offers a very friendly interface to do this in the last layers, as we can see in this snippet inspired in the book <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python from François Chollet</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv_base</span> <span class="o">=</span> <span class="nc">VGG16</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span>
    <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">set_trainable</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">conv_base</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">layer</span><span class="p">.</span><span class="n">name</span> <span class="o">==</span> <span class="s">'block5_conv1'</span><span class="p">:</span>
        <span class="n">set_trainable</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">if</span> <span class="n">set_trainable</span><span class="p">:</span>
        <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div> <p>After the modification of the body of the model, in this case called <code class="language-plaintext highlighter-rouge">convbase</code>, we can add a head block as before so its weights will be adjusted during the training process as well.</p> <p>In the particular case of the repository listed at the top of this page, we can see that on the <a href="https://www.kaggle.com/c/dogs-vs-cats">Dogs vs Cats dataset</a> the accuracy of the model is improved from 81% of precision to 90% thanks to the use of the Transfer Learning technique. Furthermore, we went even further as, thanks to the fine tuning of our model, we obtained an accuracy of 95% of precision. This may be a good proof of how easy is to use Transfer Learning in a small project, where the access to the data is expensive or impossible, so it is one of the best tools to have in your hands while working on a personal project.</p> <p>Thank you very much for your attention and keep on learning!</p>]]></content><author><name></name></author><category term="deep learning"/><category term="transfer learning"/><category term="computer vision"/><category term="python"/><category term="tensorflow"/><summary type="html"><![CDATA[Guide to achieve state of the art performance on a wide range of tasks with little to no training.]]></summary></entry><entry><title type="html">Beyond Gradient Descent Optimizer</title><link href="https://josumsc.github.io/blog/2022/why-not-vanilla-gradient-descent/" rel="alternate" type="text/html" title="Beyond Gradient Descent Optimizer"/><published>2022-07-19T01:00:00+00:00</published><updated>2022-07-19T01:00:00+00:00</updated><id>https://josumsc.github.io/blog/2022/why-not-vanilla-gradient-descent</id><content type="html" xml:base="https://josumsc.github.io/blog/2022/why-not-vanilla-gradient-descent/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-01-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-01-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-01-1400.webp"/> <img src="/assets/img/gradient-descent-01.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In almost every learning resource, books, references, or guides about Machine Learning is based on the optimization of a cost function by a learning algorithm.</p> <p>On almost every occasion, with the particular exception of the normal equation on linear regression problems, whenever we talk about optimization we are refering to the Gradient Descent algorithm. This algorithm is based on the idea of calculating the partial derivative of the different weights of the model with respect to the cost function in a point, which indicates the adjustment necessary to be performed in that weight to minimize the cost function. As the partial derivative is subject to the point in which the cost function is located, we multiply the value of the cost function by a learning rate to guide how much we will move the value of our weights.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-02-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-02-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-02-1400.webp"/> <img src="/assets/img/gradient-descent-02.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Thus this learning rate parameter, often called $\alpha$ or simply <code class="language-plaintext highlighter-rouge">lr</code>, has a huge impact on the results of our model. If we decide to make our learning rate too low, we may need more steps for our model to converge, and if we decide to make it too big, our model may not converge at all.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-03-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-03-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-03-1400.webp"/> <img src="/assets/img/gradient-descent-03.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In simple models, we can allow a low value for this function since the training times are low, but in complicated models such as deep learning where the values to update are of order hundreds or thousands, we need alternatives:</p> <h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2> <p>The very same word <em>stochastic</em> refers to the randomness of this method, which is based on selecting randomly K examples from our training set and computing the cost with them instead of with the total set, which allow us to perform updates in the weights as soon as possible and accelerate the training process. The problem of this method is its randomness, because if we had taken the pure gradient descent approach we would have taken into account all the inputs of data, but instead we are only considering a small subset of the data and adjusting the weights according to their loss, which could lead to different results and eventually lead to a non-optimal solution.</p> <h2 id="mini-batch-gradient-descent">Mini-batch Gradient Descent</h2> <p>The solution to the randomness problem on stochastic gradient descent is based on dividing our training set into K sets and adjusting the weights after each set, so that we are sure that our model will have to look at all the examples of the training set on each iteration. Each step of the full training set is called an epoch and this is a standard method in the automatic learning models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-04-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-04-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-04-1400.webp"/> <img src="/assets/img/gradient-descent-04.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="momentum-gradient-descent">Momentum Gradient Descent</h2> <p>Nevertheless, despite realizing faster adjustments thanks to the batch division of the dataset, we sometimes see that our cost function is not getting closer to the minimum, because the different sets of data may still give us results contrary to the desired result.</p> <p>In order to reduce the impact of this problem, the idea behind the momentum algorithm is to give a weight to the previous partial derivatives before performing our adjustment, which allows us to accelerate and smooth the adjustments to be made by our algorithm. The idea to follow is the same as in the smoothing of time series, reducing the impact of individual observations to get a tendency and directionality inside the function.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-05-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-05-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-05-1400.webp"/> <img src="/assets/img/gradient-descent-05.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="rmsprop">RMSProp</h2> <p>On the next iteration, the <em>Root Mean Squared Propagation</em> method appears. It corrects the negative effects of the accumulation of the values passed through the derivatives, by converting them into a moving average adjusted. Furthermore, it allow us to select a different learning rate for each weight, which is a particularly good approach for deep learning models, where we can have thousands or millions of values to optimize.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-06-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-06-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-06-1400.webp"/> <img src="/assets/img/gradient-descent-06.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="adam">ADAM</h2> <p>Finally, now that we now the basics of the momentum gradient descent and the RMSProp, we can combine the strenghts of both methods to get a more efficient optimizer that reduce the negative counterparts of selecting each of those. With this idea we reached the ADAM algorithm, which uses the quotient of the adjustments made by the two previous methods to get a more adjusted value, adding an epsilon parameter that allows to modify the weight of the momentum trend. In practice, this $/epsilon$ parameter is not modified and is left as it is on the default implementations used in libraries as <code class="language-plaintext highlighter-rouge">Keras</code> or <code class="language-plaintext highlighter-rouge">PyTorch</code>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-07-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-07-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-07-1400.webp"/> <img src="/assets/img/gradient-descent-07.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="final-remarks">Final remarks</h2> <p>Now that we have seen the different alternatives to vanilla gradient descent, it is easy to understand why we often find ourselves with one of the optimizers seen in this text more and more often. If even though our guide was not exhaustive, and there are still other optimizers to consider (such as Adagrad or Adadelta), in the practice each time more practitioners of automatic learning are enthused by ADAM, RMSProp or SGD, as those 3 are the most often seen in web competitions hosted in sites like <a href="www.kaggle.com">Kaggle</a>.</p> <p>If even after selecting an optimizer as ADAM or RMSProp you still find yourself with a sluggish model, you can consider the application of other optimization techniques such as Learning Rate Decay, which would allow us to reduce the learning rate according to the progress of the model in order to make large adjustments at the beginning and precise and small adjustments at the end:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gradient-descent-08-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gradient-descent-08-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gradient-descent-08-1400.webp"/> <img src="/assets/img/gradient-descent-08.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The combination of this technique with the use of a more advanced optimizer like ADAM allows our models and networks to evolve faster towards the desired minimum of our cost function.</p> <p>I hope the explanation was clear and that your next models will use one of the optimizers explained here. Keep reading, practicing, and experimenting, and come back to share with me your advances!</p>]]></content><author><name></name></author><category term="deep learning"/><category term="optimization"/><summary type="html"><![CDATA[What are those fancy optimizers as ADAM or RMSprop and why should I use them instead of the old trusty SGD?]]></summary></entry></feed>